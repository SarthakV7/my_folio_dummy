<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Sarthak  Vajpayee | How I used machine learning to strategize my GRE preparation.</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/my_folio/assets/img/favicon.ico">
<link rel="stylesheet" href="/my_folio/assets/css/main.css">

<link rel="canonical" href="/my_folio/blog/2020/GRE-ai/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/my_folio/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/my_folio/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/my_folio/">
       <span class="font-weight-bold">Sarthak</span>   Vajpayee
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/my_folio/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/my_folio/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/my_folio/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/my_folio/publications/">
                publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/my_folio/teaching/">
                skills
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">How I used machine learning to strategize my GRE preparation.</h1>
    <p class="post-meta">February 21, 2020</p>
  </header>

  <article class="post-content">
    <h4 id="the-most-challenging-part-of-gre-preparation-is-the-vocabulary-part-at-least-for-me-it-was-until-my-machine-learning-model-helped-me-out-with-it"><strong>The most challenging part of GRE preparation is the vocabulary part. At-least for me it was until my machine learning model helped me out with it.</strong></h4>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/5040/1*OStckVJzGb02CZt1RY8csw.png" />
    </div>
</div>

<p><img src="" alt="" />
When I started with my GRE preparation, after going through many resources (for the vocab section) I found that there are some words that pretty commonly appear in the exam and Barron’s high-frequency word list is one of the renowned resources that solve this problem. To begin with, I picked Barron’s 333 which is one such word list that contains 333 most frequently occurring words in GRE. The next challenge was learning these words so I came up with a plan. If I could somehow group similar words together it would make the learning process much easier. But how to do that? Manually grouping these words would be way more challenging than simply learning the words as they are. After pondering for some time, it occurred to me why not let the machine do all the hard work! I think with a capability of <em>above one million million floating-point operations per second</em> it is much better for these types of tasks than I am so let’s get started and see how to build a model from scratch that clusters similar words together.</p>

<p>I’ll be covering several machine learning concepts like Natural Language Processing (<strong>NLP</strong>), Term Frequency-Inverse Document Frequency (<strong>TF-IDF</strong>), Singular Value Decomposition (<strong>SVD</strong>), <strong>K-Means</strong>, t-Distributed Stochastic Neighbor Embedding (<strong>t</strong>-<strong>SNE</strong>) and many other techniques for data scraping, feature engineering and data visualization to demonstrate how we can cluster data from scratch.</p>

<blockquote>
  <p><strong>Note: I’ll be using python 3.7 for this project.</strong></p>
</blockquote>

<p>The blog will be divided into the following parts-</p>

<ul>
  <li>
    <p>Data collection: scraping websites to gather the data.</p>
  </li>
  <li>
    <p>Data cleaning</p>
  </li>
  <li>
    <p>Feature engineering</p>
  </li>
  <li>
    <p>Modeling</p>
  </li>
  <li>
    <p>Visualizing the results</p>
  </li>
</ul>

<p>Now that we know the problem statement and the data flow, let’s dive in.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Scraping the data</strong></p>
</blockquote>

<p>The first task is to collect the data i.e. Barron’s 333 high-frequency words. This can be done either by manually typing the words and creating a list or by automating the process. I used BeaulifulSoup and request to create a function that automatically scraped the data from different websites, let’s briefly understand the libraries and how to use them.</p>

<ul>
  <li>
    <p><strong><em>Numpy:</em></strong> A library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.</p>
  </li>
  <li>
    <p><strong><em>Pandas:</em></strong> A library written for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables.</p>
  </li>
  <li>
    <p><strong><em>BeautifulSoup:</em></strong> A library for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.</p>
  </li>
  <li>
    <p><strong><em>Requests:</em></strong> The requests module allows you to send HTTP requests using Python. The HTTP request returns a response object with all the response data (content, encoding, status, etc).</p>
  </li>
</ul>

<p>The code will use <strong>requests</strong> to get the response from the target websites, then using <strong>BeautifulSoup</strong> it’ll parse the html response and scrape out the required information from the page(s) and store the information in a tabular format using <strong>pandas.</strong>
To understand the format of an html page, you can check out <a href="https://www.w3schools.com/html/">this tutorial</a>.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># importing necessary libraries

import requests
from bs4 import BeautifulSoup
import re
from functools import reduce
import numpy as np
import pandas as pd
</code></pre></div></div>

<p>Let’s scrape the Barron’s 333 words and their meanings from <a href="https://quizlet.com/2832581/barrons-333-high-frequency-words-flash-cards/">this website</a>-</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre></td><td class="code"><pre><span class="n">URL</span> <span class="o">=</span> <span class="s">"https://quizlet.com/2832581/barrons-333-high-frequency-words-flash-cards/"</span> <span class="c1"># url of the data we want to scrape.
</span><span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">URL</span><span class="p">)</span> <span class="c1"># request object collects server's response to the http request.
</span><span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">content</span><span class="p">,</span> <span class="s">'html5lib'</span><span class="p">)</span> <span class="c1"># BeautifulSoup creates a parser tree out of the html response that was collected using request.
</span><span class="n">rows</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="n">find_all</span><span class="p">(</span><span class="s">'div'</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="s">'SetPageTerm-inner'</span><span class="p">)</span> <span class="c1"># Looking for elements with tag='div' and class_='SetPageTerm-inner'.
</span><span class="n">dic</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">:</span> <span class="c1"># iterating over all the elements.
</span>    <span class="n">part</span> <span class="o">=</span> <span class="n">row</span><span class="p">.</span><span class="n">find_all</span><span class="p">(</span><span class="s">'span'</span><span class="p">,</span> <span class="n">class_</span><span class="o">=</span><span class="s">'TermText notranslate lang-en'</span><span class="p">)</span> <span class="c1"># Looking for elements with tag='span' and class_='TermText notranslate lang-en'
</span>    <span class="n">word</span> <span class="o">=</span> <span class="n">part</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">text</span> <span class="c1"># collecting the words.
</span>    <span class="n">meaning</span> <span class="o">=</span> <span class="n">part</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">text</span> <span class="c1"># collecting the meaning.
</span>    <span class="n">dic</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">meaning</span> <span class="c1"># adding the word, meaning to dictionary as key value pairs.
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">dic</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'word'</span><span class="p">,</span> <span class="s">'meaning'</span><span class="p">])</span> <span class="c1"># converting to dataframe
</span> 
</pre></td></tr></tbody></table></code></pre></figure>

<h1 id="the-html-code-of-a-webpage-in-chrome-can-be-accessed-using-shiftc-on-mac-or-ctrlshiftc-on-windowslinux">The HTML code of a webpage in chrome can be accessed using <strong>⌘+shift+c</strong> on mac or <strong>ctrl+shift+c</strong> on windows/Linux.</h1>

<p>Here I’m using ‘span’ as tag, class_= ‘TermText notranslate lang-en’ since the elements containing word and meaning have the same tag and class and there are only 2 such elements in every row element, 1st one corresponding to word and the 2nd one to the meaning.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3852/1*TlHPuTuo-vnLXl1Y__H-KQ.png" />
    </div>
</div>

<p><img src="" alt="" />
This is the scraped data in tabular form.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*B1evIg_8BvaDpMiHYwqLPw.png" />
    </div>
</div>

<p><img src="" alt="" />
This data is not enough so let’s add more data by scraping the synonyms of each word from <a href="https://www.thesaurus.com/browse/">this website</a>-</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="code"><pre><span class="k">def</span> <span class="nf">synonyms</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">th</span><span class="o">=</span><span class="mi">20</span><span class="p">):</span>
  <span class="n">URL</span> <span class="o">=</span> <span class="sa">f</span><span class="s">"https://www.thesaurus.com/browse/</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s">"</span> <span class="c1"># this url returns the page with 'word' described
</span>  <span class="n">r</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">URL</span><span class="p">)</span> <span class="c1"># collecting the http response from the url
</span>  <span class="n">soup</span> <span class="o">=</span> <span class="n">BeautifulSoup</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">content</span><span class="p">,</span> <span class="s">'html5lib'</span><span class="p">)</span> <span class="c1"># parsing the html page to extract items.
</span>  <span class="n">rows</span> <span class="o">=</span> <span class="n">soup</span><span class="p">.</span><span class="n">find_all</span><span class="p">(</span><span class="s">'span'</span><span class="p">,</span> <span class="n">class_</span> <span class="o">=</span> <span class="s">'css-133coio etbu2a32'</span><span class="p">)</span> <span class="c1"># stores all the element containing synonyms of the word.
</span>  <span class="n">syn</span> <span class="o">=</span> <span class="p">[</span><span class="n">word</span><span class="p">]</span>
  <span class="k">if</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span><span class="o">&lt;</span><span class="n">th</span><span class="p">):</span> <span class="c1"># here, th denotes the number of synonyms that we need.
</span>    <span class="n">th</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rows</span><span class="p">)</span> <span class="c1"># in some cases only limited synonyms were available so I modified the code a bit.
</span>  <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">rows</span><span class="p">[:</span><span class="n">th</span><span class="p">]:</span> <span class="c1"># iterating over all the elements containing synonyms
</span>    <span class="k">try</span><span class="p">:</span>
      <span class="n">syn</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">a</span><span class="p">.</span><span class="n">text</span><span class="p">)</span> <span class="c1"># scrapping synonym string and storing in a list (most of them have a tag='a')
</span>    <span class="k">except</span><span class="p">:</span>
      <span class="n">syn</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span><span class="p">.</span><span class="n">span</span><span class="p">.</span><span class="n">text</span><span class="p">)</span> <span class="c1"># scrapping synonym string with tag='span'
</span>  <span class="k">return</span> <span class="n">syn</span>

<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm_notebook</span>
<span class="n">mapping</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># stores the word and it's synonyms
</span><span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tqdm_notebook</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">word</span><span class="p">.</span><span class="n">values</span><span class="p">):</span> <span class="c1"># iterating over the words and scrapping synonyms
</span>    <span class="n">syn</span> <span class="o">=</span> <span class="n">synonyms</span><span class="p">(</span><span class="n">word</span><span class="p">,</span> <span class="n">th</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span> <span class="c1"># I'll be collecting 5 synonyms
</span>    <span class="n">mapping</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">syn</span><span class="p">)</span> <span class="c1"># storing the synonyms.
</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">mapping</span><span class="p">)</span> <span class="c1"># Converting to dataframe
</span><span class="n">data</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'word'</span><span class="p">,</span><span class="s">'synonym_1'</span><span class="p">,</span><span class="s">'synonym_2'</span><span class="p">,</span><span class="s">'synonym_3'</span><span class="p">,</span><span class="s">'synonym_4'</span><span class="p">,</span><span class="s">'synonym_5'</span><span class="p">]</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4976/1*3jfzEZeasCMd6uKm8GDe6g.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*PLr0hPzITQY5pY5JH_XO2g.png" />
    </div>
</div>

<p><img src="" alt="" />
Now let’s join the 2 data frames (meanings and synonyms):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>result = pd.merge(df.word, data, on='word')
result.fillna('', inplace=True)
print(result)
</code></pre></div></div>

<p><img src="" alt="" /></p>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2840/1*iPHWY7srA9FlwLVrLIxUWg.png" />
    </div>
</div>

<p><img src="" alt="" />
We can see the data needs some cleaning since it contains stop-words like and, or, the and other elements like punctuation marks. Also, we must take care of the contractions like can’t, won’t, don’t these must be converted to can not, would not, do not respectively.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre></td><td class="code"><pre><span class="kn">from</span> <span class="nn">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>
<span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="n">sentence</span><span class="p">):</span>
  <span class="n">stop_words</span> <span class="o">=</span> <span class="p">[</span><span class="s">'ourselves'</span><span class="p">,</span> <span class="s">'hers'</span><span class="p">,</span> <span class="s">'between'</span><span class="p">,</span> <span class="s">'yourself'</span><span class="p">,</span> <span class="s">'but'</span><span class="p">,</span> <span class="s">'again'</span><span class="p">,</span> <span class="s">'there'</span><span class="p">,</span> <span class="s">'about'</span><span class="p">,</span> <span class="s">'once'</span><span class="p">,</span> <span class="s">'during'</span><span class="p">,</span>
              <span class="s">'out'</span><span class="p">,</span> <span class="s">'very'</span><span class="p">,</span> <span class="s">'having'</span><span class="p">,</span> <span class="s">'with'</span><span class="p">,</span> <span class="s">'they'</span><span class="p">,</span> <span class="s">'own'</span><span class="p">,</span> <span class="s">'an'</span><span class="p">,</span> <span class="s">'be'</span><span class="p">,</span> <span class="s">'some'</span><span class="p">,</span> <span class="s">'for'</span><span class="p">,</span> <span class="s">'do'</span><span class="p">,</span> <span class="s">'its'</span><span class="p">,</span> <span class="s">'yours'</span><span class="p">,</span>
              <span class="s">'such'</span><span class="p">,</span> <span class="s">'into'</span><span class="p">,</span> <span class="s">'of'</span><span class="p">,</span> <span class="s">'most'</span><span class="p">,</span> <span class="s">'itself'</span><span class="p">,</span> <span class="s">'other'</span><span class="p">,</span> <span class="s">'off'</span><span class="p">,</span> <span class="s">'is'</span><span class="p">,</span> <span class="s">'s'</span><span class="p">,</span> <span class="s">'am'</span><span class="p">,</span> <span class="s">'or'</span><span class="p">,</span> <span class="s">'who'</span><span class="p">,</span> <span class="s">'as'</span><span class="p">,</span>
              <span class="s">'from'</span><span class="p">,</span> <span class="s">'him'</span><span class="p">,</span> <span class="s">'each'</span><span class="p">,</span> <span class="s">'the'</span><span class="p">,</span> <span class="s">'themselves'</span><span class="p">,</span> <span class="s">'until'</span><span class="p">,</span> <span class="s">'below'</span><span class="p">,</span> <span class="s">'are'</span><span class="p">,</span> <span class="s">'we'</span><span class="p">,</span> <span class="s">'these'</span><span class="p">,</span> <span class="s">'your'</span><span class="p">,</span>
              <span class="s">'his'</span><span class="p">,</span> <span class="s">'through'</span><span class="p">,</span> <span class="s">'don'</span><span class="p">,</span> <span class="s">'nor'</span><span class="p">,</span> <span class="s">'me'</span><span class="p">,</span> <span class="s">'were'</span><span class="p">,</span> <span class="s">'her'</span><span class="p">,</span> <span class="s">'more'</span><span class="p">,</span> <span class="s">'himself'</span><span class="p">,</span> <span class="s">'this'</span><span class="p">,</span> <span class="s">'down'</span><span class="p">,</span> <span class="s">'should'</span><span class="p">,</span>
              <span class="s">'our'</span><span class="p">,</span> <span class="s">'their'</span><span class="p">,</span> <span class="s">'while'</span><span class="p">,</span> <span class="s">'above'</span><span class="p">,</span> <span class="s">'both'</span><span class="p">,</span> <span class="s">'up'</span><span class="p">,</span> <span class="s">'to'</span><span class="p">,</span> <span class="s">'ours'</span><span class="p">,</span> <span class="s">'had'</span><span class="p">,</span> <span class="s">'she'</span><span class="p">,</span> <span class="s">'all'</span><span class="p">,</span> <span class="s">'no'</span><span class="p">,</span>
              <span class="s">'when'</span><span class="p">,</span> <span class="s">'at'</span><span class="p">,</span> <span class="s">'any'</span><span class="p">,</span> <span class="s">'before'</span><span class="p">,</span> <span class="s">'them'</span><span class="p">,</span> <span class="s">'same'</span><span class="p">,</span> <span class="s">'and'</span><span class="p">,</span> <span class="s">'been'</span><span class="p">,</span> <span class="s">'have'</span><span class="p">,</span> <span class="s">'in'</span><span class="p">,</span> <span class="s">'will'</span><span class="p">,</span> <span class="s">'on'</span><span class="p">,</span> <span class="s">'does'</span><span class="p">,</span>
              <span class="s">'yourselves'</span><span class="p">,</span> <span class="s">'then'</span><span class="p">,</span> <span class="s">'that'</span><span class="p">,</span> <span class="s">'because'</span><span class="p">,</span> <span class="s">'what'</span><span class="p">,</span> <span class="s">'over'</span><span class="p">,</span> <span class="s">'why'</span><span class="p">,</span> <span class="s">'so'</span><span class="p">,</span> <span class="s">'can'</span><span class="p">,</span> <span class="s">'did'</span><span class="p">,</span> <span class="s">'not'</span><span class="p">,</span> <span class="s">'now'</span><span class="p">,</span>
              <span class="s">'under'</span><span class="p">,</span> <span class="s">'he'</span><span class="p">,</span> <span class="s">'you'</span><span class="p">,</span> <span class="s">'herself'</span><span class="p">,</span> <span class="s">'has'</span><span class="p">,</span> <span class="s">'just'</span><span class="p">,</span> <span class="s">'where'</span><span class="p">,</span> <span class="s">'too'</span><span class="p">,</span> <span class="s">'only'</span><span class="p">,</span> <span class="s">'myself'</span><span class="p">,</span> <span class="s">'which'</span><span class="p">,</span> <span class="s">'those'</span><span class="p">,</span>
              <span class="s">'i'</span><span class="p">,</span> <span class="s">'after'</span><span class="p">,</span> <span class="s">'few'</span><span class="p">,</span> <span class="s">'whom'</span><span class="p">,</span> <span class="s">'t'</span><span class="p">,</span> <span class="s">'being'</span><span class="p">,</span> <span class="s">'if'</span><span class="p">,</span> <span class="s">'theirs'</span><span class="p">,</span> <span class="s">'my'</span><span class="p">,</span> <span class="s">'against'</span><span class="p">,</span> <span class="s">'a'</span><span class="p">,</span> <span class="s">'by'</span><span class="p">,</span> <span class="s">'doing'</span><span class="p">,</span> <span class="s">'it'</span><span class="p">,</span>
              <span class="s">'how'</span><span class="p">,</span> <span class="s">'further'</span><span class="p">,</span> <span class="s">'was'</span><span class="p">,</span> <span class="s">'here'</span><span class="p">,</span> <span class="s">'than'</span><span class="p">]</span>
  <span class="n">sentence_clean</span> <span class="o">=</span> <span class="p">[</span><span class="n">words</span> <span class="k">if</span> <span class="n">words</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span> <span class="k">else</span> <span class="s">''</span> <span class="k">for</span> <span class="n">words</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">)]</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">sentence_clean</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">';'</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'\('</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'\)'</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">','</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'-'</span><span class="p">,</span> <span class="s">' '</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'\d'</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'  '</span><span class="p">,</span> <span class="s">' '</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"won't"</span><span class="p">,</span> <span class="s">"will not"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"can\'t"</span><span class="p">,</span> <span class="s">"can not"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"n\'t"</span><span class="p">,</span> <span class="s">" not"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'re"</span><span class="p">,</span> <span class="s">" are"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'s"</span><span class="p">,</span> <span class="s">" is"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'d"</span><span class="p">,</span> <span class="s">" would"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'ll"</span><span class="p">,</span> <span class="s">" will"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'t"</span><span class="p">,</span> <span class="s">" not"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'ve"</span><span class="p">,</span> <span class="s">" have"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="n">sentence</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'m"</span><span class="p">,</span> <span class="s">" am"</span><span class="p">,</span> <span class="n">sentence</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">sentence</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now since the data has been cleaned, let’s do some feature engineering.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Feature engineering</strong></p>
</blockquote>

<p>One thing that I noticed was that some of these words have synonyms that also belong to Barron’s 333 list. If I could somehow concatenate the synonyms of these words, it could increase the performance of our model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*m7RGuamAlyh_gNf_ACvY-A.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>Eg. for <strong><em>Tortuous</em></strong>, the synonyms are <em>circuitous, convoluted, indirect.</em></p>
  </li>
  <li>
    <p>Out of the 3 synonyms, <strong>convoluted</strong> is present in Barron’s 333-word list and its synonyms are <em>intricate, labyrinthine, perplexing.</em></p>
  </li>
  <li>
    <p>So, the final synonyms of <strong>tortuous</strong> should be <em>circuitous, convoluted, indirect, intricate, labyrinthine, perplexing</em>.</p>
  </li>
</ul>

<p>After this step, for each word in Barron’s 333-word list we have, it’s direct synonyms, indirect synonyms (in the above example, the synonyms of <strong>convoluted</strong> are the indirect synonyms of <strong>tortuous</strong>) and meaning. I’ll use the notation <strong>set</strong> for this data further in this blog.</p>
<blockquote>
  <p><strong>Set:</strong> data about a word (here the word is from barron’s 333) like it’s direct synonyms, indirect synonyms and meaning. The set of a word includes the word itself.</p>
</blockquote>

<p>We have obtained clean sets of words that we need to cluster but remember we first need to convert these sets into some kind of numerical data because our model needs numbers to work on.</p>

<p>I’ll use <strong>TF-IDF</strong> to vectorize the data. Before diving in let’s understand what tf-idf is-</p>

<p><strong>TF-IDF</strong> short for term frequency-inverse document frequency is a numerical statistic that is intended to reflect how important a word is to a document in a corpus. Let’s understand this using <strong>Bag of Words</strong>.</p>

<p>The <strong>bag-of-words</strong> model is a simplifying representation used in natural language processing and information retrieval. In this model, a text is represented as the bag of its words.
In simple terms, Bag of Words is nothing but a basic numerical representation of documents, it is done by first creating a <strong>vocabulary</strong> of words that contains all the distinct words from all the documents. Now each document is represented using a vector that has ‘n’ elements (here, n is the number of words in the vocabulary so each element corresponds to a word in the vocabulary) and each element has a numerical value that tells us how many times that word was seen in that document.
let’s consider an example:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*EVu8iK6PijpfCxqjhFRpaQ.jpeg" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>The column <strong>word</strong> represents the vocabulary.</p>
  </li>
  <li>
    <p>In the table, column Document 1 and Document 2 represent the BOW of documents 1 and 2 respectively.</p>
  </li>
  <li>
    <p>The numbers represent how many times the corresponding word occurs in the document.</p>
  </li>
  <li>
    <p>Now comes TF-IDF, it is simply the product of term frequency and inverse document frequency.</p>
  </li>
</ul>

<p><em><strong>Term frequency</strong>:</em> It is the number that represents how often a word is present in the document.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*rQlC66HYBiVPYOXbUl1tvQ.png" />
    </div>
</div>

<p><img src="" alt="" />
<em><strong>Inverse document frequency</strong>:</em> It is the inverse of the log of the chance of finding a document that has the word in it.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*xtgXRlC-gSmoFOIG2vrSTQ.png" />
    </div>
</div>

<p><img src="" alt="" />
Think about it this way: If the word is used extensively in all documents, its existence within a specific document will not be able to provide as much specific information about the document itself. So the second term could be seen as a penalty term that penalizes common words such as “a”, “the”, “and”, etc. tf-idf can, therefore, be seen as a weighting scheme for words relevancy in a specific document.</p>

<p>Let’s check the TF-IDF of the two documents:
Document 1: “tf stands for term frequency”, terms: 5
Document 2: “and idf stands for inverse document frequency”, terms: 7</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3056/1*CzVrAaxzaLfs0bF8ZbkLSA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2252/1*bueWekM8K0tTHXRMI_0xSg.png" />
    </div>
</div>

<p><img src="" alt="" />
<strong><em>Bi-grams:</em></strong> A bigram is a sequence of two adjacent elements from a string of tokens, which are typical letters, syllables, or words. Here is an example of uni-grams and bi-grams generated from a document.
<strong><em>doc:</em></strong> “tf stands for term frequency”
<em>uni-grams:</em> [‘tf’, ‘stands’, ‘for’, ‘term’, ‘frequency’]
<em>bi-grams:</em> [‘tf stands’, ‘stands for’, ‘for term’, ‘term frequency’]</p>

<p><em>The advantage of n-grams is that they add information about the sequence of words in a document.</em></p>

<p>I’ve written a function for calculating the TF-IDF, the function uses both uni-grams and bi-grams.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">tfidf_vectorizer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_grams</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tf</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">idf</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tfidf_</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
      <span class="s">'''This function generates the vocabulary.
      Here I've used both uni-grams and bi-grams.'''</span>
        <span class="n">uni_grams</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="n">bi_grams</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">rows</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">rows</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word_pair</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">words</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                <span class="n">uni_grams</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                <span class="n">bi_grams</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">word_pair</span><span class="p">)</span>
            <span class="n">uni_grams</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_grams</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">uni_grams</span><span class="p">.</span><span class="n">union</span><span class="p">(</span><span class="n">bi_grams</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
      <span class="s">'''This function calculates the tf values for each document, idf values for each word
      and then finally returns the tfidf values'''</span>
        <span class="n">tf_</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="bp">self</span><span class="p">.</span><span class="n">n_grams</span><span class="p">])</span>
        <span class="n">idf_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n_grams</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">idf_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n_grams</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span><span class="n">rows</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="n">words</span> <span class="o">=</span> <span class="n">rows</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">' '</span><span class="p">)</span>
            <span class="n">tf</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">.</span><span class="n">fromkeys</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n_grams</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word_pair</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">words</span><span class="p">[</span><span class="mi">1</span><span class="p">:]):</span>
                <span class="n">tf</span><span class="p">[</span><span class="n">word_pair</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">tf</span><span class="p">[</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">idf_</span><span class="p">[</span><span class="n">word_pair</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="n">idf_</span><span class="p">[</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">tf</span><span class="p">[</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">idf_</span><span class="p">[</span><span class="n">word_pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">idf_list</span> <span class="o">+=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">idf_</span><span class="p">.</span><span class="n">values</span><span class="p">()))</span>
            <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">values</span><span class="p">()))</span>
            <span class="n">vector</span> <span class="o">=</span> <span class="n">vector</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span>
            <span class="n">tf_</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">vector</span>
        <span class="c1"># print(idf_list)
</span>        <span class="n">idf_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">/</span><span class="n">term</span><span class="p">)</span> <span class="k">for</span> <span class="n">term</span> <span class="ow">in</span> <span class="n">idf_list</span><span class="p">])</span>
        <span class="n">idf_</span> <span class="o">=</span> <span class="n">nz</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">idf_</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">tfidf_</span> <span class="o">=</span> <span class="n">tf_</span><span class="p">.</span><span class="n">values</span><span class="o">*</span><span class="n">idf_</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">tf</span> <span class="o">=</span> <span class="n">tf_</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">idf</span> <span class="o">=</span> <span class="n">idf_</span>
        <span class="k">return</span> <span class="n">tfidf_</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
      <span class="s">'''This function performs both fit and transform'''</span>
        <span class="n">fit_</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="n">transform_</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">transform_</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now, that we have the TF-IDF embeddings for each of the sets, we can proceed to modeling.</p>

<p>Note: The data we have till now is in a tabular form containing <strong>m rows</strong> and <strong>n columns</strong> where <strong>‘m’ is the number of words in Barron’s 333-word list and ‘n’ is the size of Bag of Words vocabulary. This tabular data can also be represented as an array of dimension (m x n).</strong> Further in the blog, I’ll be using array to represent the data.</p>

<p>Before Modeling there is something that I would like to share that I found very helpful. Instead of using the TF-IDF values directly for modeling, how about bringing down the dimensions of the data?
That means, we have TF-IDF values corresponding to each set and since these TF-IDF values are represented as a vector of n elements, where ‘n’ also corresponds to the number of distinct words in all of our sets. If 2 sets have almost the same words, the distance between there corresponding points in an n-dimensional hyperplane is going to be very less and vice-versa. Similarly, if the 2 sets have very less words in common, the distance between there corresponding points in n-hyperplane is going to be much more and vice-versa. Now instead of using n-hyperplane to represent these points, I reduced the dimensionality of the points to 32-dimensions (Why 32? is discussed later in the blog). The dimensionality can be reduced by picking 32 random dimensions and ignoring the others but that would just be too stupid, so I tried using different dimensionality reduction techniques and found Truncated SVD to work miracles for the given data.</p>
<blockquote>
  <p>I used dimensionality reduction as a method to add some form of <a href="https://en.wikipedia.org/wiki/Regularization_(mathematics)">regularization</a> to the data.</p>
</blockquote>

<p><strong>Let’s understand Truncated SVD-</strong></p>

<p>SVD abbreviation for Singular Value Decomposition is a matrix factorization technique that factorizes any given matrix into the three matrices U, S, and V.
It goes by the equation-</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2900/0*ieoRDguEa0BVrLyO.jpeg" />
    </div>
</div>

<p><img src="" alt="" />
Let me explain what U, S, and V are.
<strong><em>U (aka left singular):</em></strong> is an orthogonal matrix whose columns are the eigenvectors of <strong>AᵀA</strong>.
<strong><em>S (singular):</em></strong> is a diagonal matrix whose diagonal elements are the square root of eigenvalues of <strong>AᵀA</strong> or <strong>AAᵀ</strong>(both have the same Eigenvalues) arranged in descending order i.e.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2792/1*rVknwpIrbPkFOIyemab5HA.png" />
    </div>
</div>

<p><strong><em>V (aka right singular):</em></strong> is an orthogonal matrix whose columns are the eigenvectors of <strong>AAᵀ</strong><em>.</em></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2712/0*mJ12be_KbhuS8Ta0.jpeg" />
    </div>
</div>

<p><img src="" alt="" />
<strong>Eigenvectors:</strong> An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it. Consider the image below in which three vectors are shown. The green square is only drawn to illustrate the linear transformation that is applied to each of these three vectors.
Note that the direction of these eigenvectors do not change but their length does change, and <strong>eigenvalue</strong> is the factor by which their length change.
source: <a href="https://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/">https://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/</a></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*7SkivPxZSwPO1NsQ.png" />
    </div>
</div>

<p><img src="" alt="" />
So now that we know how to factorize a matrix, we can reduce the dimension of a matrix using Truncated SVD which is a simple extension to SVD.</p>

<p><strong>Truncated SVD:</strong> Suppose we have an input matrix of dimensions (m x n) which we want to reduce to (m x r) where r&lt;n.
We simply compute the first <strong>‘r’</strong> eigenvectors of <strong>AᵀA</strong> and store it as the columns of U, then we compute the first <strong>‘r’</strong> eigenvectors of <strong>AAᵀ</strong> and store it as the columns of V and finally the root of first <strong>‘r’</strong> eigenvalues as diagonal elements of S.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2712/0*ajW4rgk6fhrYmNG_.gif" />
    </div>
</div>

<p><img src="" alt="" />
So in a nutshell, Truncated-SVD is a smart technique that reduces the dimensionality of given data in a smart way by preserving as much information (variance) as possible.</p>

<p>If you want to dive deeper into SVD, check out <a href="https://youtu.be/Nx0lRBaXoz4">this lecture</a> by Prof. W. Gilbert Strang, and <a href="https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491">this wonderful blog</a> on SVD.</p>

<p>Scikit-learn comes with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html">Truncated SVD</a> built in that can be imported and used directly.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Modeling</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2880/0*4gydrw9A3FqJfb7m.png" />
    </div>
</div>

<p>Now that we are done with the data pre-processing and feature engineering part, let’s see how to group similar words together using a clustering algorithm.</p>

<p><img src="" alt="" /></p>
<h2 id="k-means">K-Means</h2>

<p><strong>K</strong>-<strong>means</strong> clustering is a type of unsupervised learning, which is used when you have unlabeled data (i.e., data without defined categories or groups). The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable <strong>K</strong>.</p>

<p>Let’s see how K-means works.
Suppose there are some points in a 2-dimensional plane, and we want to cluster these points into <strong>K</strong> clusters.</p>

<div class="row mt-3">
     <div class="col-sm mt-3 mt-md-1">
         <img class="img-fluid rounded z-depth-0" src="https://lh3.googleusercontent.com/proxy/nEQUf8QVAdV7TSAW_kzeLOeKgpE0q1WRsL0BM-01OHsiGOOx8RPR4LFD6qm2Pw8KDlYxQkzthNlJBmLO4fGtSG41uRT2Oyw_i1jh" />
     </div>
 </div>

<p>The steps are simple-</p>

<ul>
  <li>
    <p>Defining <strong>K</strong> points randomly in the plane. Let’s call these points as cluster <strong>centroids</strong>.</p>
  </li>
  <li>
    <p>Iterating over each point in the data and check for the closest centroid and assign that point to its closest centroid.</p>
  </li>
  <li>
    <p>After the above step, each centroid must have some points that are closest to it, let’s call these sets of points <strong>clusters</strong>.</p>
  </li>
  <li>
    <p>Updating the centroid of each cluster by calculating the mean value of x and y coordinate of all the points in that cluster. The calculated mean values (x, y) are the coordinates of the updated centroid of that cluster.</p>
  </li>
  <li>
    <p>Repeating the last 3 steps until the coordinates of the centroids do not update much.</p>
  </li>
</ul>

<p><strong>But how to decide the right value for K (number of clusters)?</strong>
Let’s define a metric that can be used to measure the right value of <strong>K.</strong>
Distortion is one such metric that uses the sum of squares of the distance of points in a cluster from the cluster mean, for all the clusters summed up.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2048/1*aSeXkf9At7WSbGy5s_d4vw.png" />
    </div>
</div>

<p><img src="" alt="" />
Distortion can be used to check how efficient the clustering algorithm is for a given value of <strong>K</strong>.
The optimal value of K can be determined by calculating the distortion value for different values of K and then plotting them.
This plot is known as the <strong>Elbow plot</strong>. Just by looking at the elbow plot we can determine the optimal ‘K’ as the value where distortion stops decreasing rapidly.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*OIKw1kVoK0GVfDiB.png" />
    </div>
</div>

<p><img src="" alt="" />
Here is an Elbow plot and just by looking at it, we can say that the best hyperparameter is k=3.</p>

<p>It is called elbow plot because it looks like an arm (maybe a stick man’s arm) and the elbow of this arm represents the optimal K.</p>

<p>One more thing, I’ll be using cosine distance as a measure to compute the distance between points (including centroids). Let’s quickly understand what <strong>cosine distance</strong> is using <em>cosine similarity.</em></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*1Z6SiZT4cIUj05Hk" />
    </div>
</div>

<p><img src="" alt="" />
<strong>Cosine similarity</strong> is a measure to calculate how parallel 2 vectors are. It is calculated using the cosine of the angle between 2 vectors. It can easily be calculated using-</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*NJpAgmY9Xml5RPeY" />
    </div>
</div>

<p><img src="" alt="" />
So if 2 vectors a and b are parallel, the angle between them will be 0 and the cosine similarity will be cos(0) = 1. Similarly, if 2 vectors a and b are pointing in the opposite direction, the angle between them will be 𝛑 and the cosine similarity will be cos(𝛑) = -1.
In a nutshell, cosine similarity tells us about what extent 2 vectors are pointing in a similar direction. A value near 1 tells us that the vectors are pointing in a very similar direction whereas a value near -1 corresponds to pointing in opposite directions.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*GVHoeYeb-P8nySnEQo8JkQ.png" />
    </div>
</div>

<p><img src="" alt="" />
Now <strong>Cosine distance</strong> between 2 points is nothing but
<strong><em>1 - (cosine similarity of the vectors representing them in hyperspace)</em></strong>.
So it ranges from (0 to 2), where 0 corresponds to the points being very similar, and 2 corresponds to the points being very dissimilar.
Can you guess when will the cosine distance between 2 points be 1?</p>

<p>To know more check out <a href="https://www.machinelearningplus.com/nlp/cosine-similarity/">this blog</a>.</p>

<p>I’ll be implementing K-means from scratch since Scikit learns K-Means does not support cosine distance.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
</pre></td><td class="code"><pre><span class="k">class</span> <span class="nc">KMeans</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_clusters</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">n_clusters</span> <span class="o">=</span> <span class="n">n_clusters</span> <span class="c1"># initializing the number of clusters
</span>
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span> <span class="c1"># This function performs fit operation on the data
</span>        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">clusters</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="c1"># initializing the clusters as all zeros
</span>
        <span class="c1"># initializing centroids
</span>        <span class="n">rows</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span>
        <span class="n">rows</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">centroids</span> <span class="o">=</span> <span class="n">rows</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">n_clusters</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

        <span class="c1"># Initialize old centroids as all zeros
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">old_centroids</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">n_clusters</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])),</span>
                                          <span class="n">columns</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>

        <span class="c1"># check the distance of each data point to the centroid and assigning each point to the closest cluster.
</span>        <span class="k">while</span> <span class="ow">not</span> <span class="bp">self</span><span class="p">.</span><span class="n">old_centroids</span><span class="p">.</span><span class="n">equals</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">):</span>
            <span class="c1"># Stash old centroids
</span>            <span class="bp">self</span><span class="p">.</span><span class="n">old_centroids</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

            <span class="c1"># Iterate through each data point/set
</span>            <span class="k">for</span> <span class="n">row_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">)):</span>
                <span class="n">distances</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
                <span class="n">point</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">row_i</span><span class="p">]</span>

                <span class="c1"># Calculate the distance between the point and centroid
</span>                <span class="k">for</span> <span class="n">row_c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">)):</span>
                    <span class="n">centroid</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">row_c</span><span class="p">]</span>
                    <span class="n">point_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">point</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">centroid_array</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">centroid</span><span class="p">).</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                    <span class="n">distances</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">cosine_distances</span><span class="p">(</span><span class="n">point_array</span><span class="p">,</span> <span class="n">centroid_array</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">])</span>

                <span class="c1"># Assign this data point to a cluster
</span>                <span class="bp">self</span><span class="p">.</span><span class="n">clusters</span><span class="p">[</span><span class="n">row_i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">))</span>

            <span class="c1"># For each cluster extract the values which now belong to each cluster and calculate new k-means
</span>            <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">n_clusters</span><span class="p">):</span>

                <span class="n">label_idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">clusters</span> <span class="o">==</span> <span class="n">label</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_idx</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">old_centroids</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Set the new centroid to the mean value of the data points within this cluster
</span>                    <span class="bp">self</span><span class="p">.</span><span class="n">centroids</span><span class="p">.</span><span class="n">loc</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">label_idx</span><span class="p">].</span><span class="n">mean</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<p>It’s time to run the algorithm on the pre-processed data and check for the right hyperparameters.
Hyperparameter tuning is a process of determining the right hyperparameters that make the model work phenomenally well for the given data.
In this case, there are 2 hyperparameters-</p>

<ul>
  <li>
    <p>1 from Truncated-SVD: n_components (reduced dimension)</p>
  </li>
  <li>
    <p>1 from K-means: ‘K’ (number of clusters).</p>
  </li>
</ul>

<p>I’ve already plotted the elbow plots for different n_component values.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2304/1*pMWankiqTj-wyeWiCftndQ.png" />
    </div>
</div>

<p><img src="" alt="" />
By looking at the plots, we can say that the best hyperparameters are-</p>
<ul>
  <li>n_components: 32</li>
  <li>K (number of clusters): 50</li>
</ul>

<p>Finally, It’s time to initialize Truncated-SVD and K-Means using the best hyperparameters and cluster the data.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from sklearn.decomposition import TruncatedSVD

&gt; trans = TruncatedSVD(n_components=32)
&gt; data_updated = trans.fit_transform(words_tfidf.toarray())
&gt; model = custom_KMeans(n_clusters=50)
&gt; model.train(data_updated)
</code></pre></div></div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Visualizing the results</strong></p>
</blockquote>

<p>These are the results obtained after clustering the data.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3200/1*UockM02CJAXyktGngP2EyA.png" />
    </div>
</div>

<p><img src="" alt="" />
Let’s check out some of the clusters:
I’ll be using the <a href="https://networkx.github.io/documentation/stable/">networkx library</a> to create the clusters.
In each cluster, the red nodes correspond to the words from Barron’s 333-word list and how they are linked with each other and their synonyms.
You can check out the documentation for networkx <a href="https://networkx.github.io/documentation/stable/">here</a>. Also, I’ll demonstrate how wonderful it is with an example in the end.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/6022/1*qzIVJO_m0UUOemKXinmGgQ.png" />
    </div>
</div>

<p>Finally, let’s visualize the data in 3d using t-SNE but first,</p>

<p><img src="" alt="" />
<strong>Let’s talk about t-SNE:</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2560/0*erGNe7ChcAcpJzdX.jpg" />
    </div>
</div>

<p><img src="" alt="" />
<strong>t-SNE</strong> (t-Distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction algorithm used for exploring high-dimensional data. It maps multi-dimensional data to two or more dimensions such that each embedding in the lower dimension represents the value in higher dimension. Also, these embeddings are placed in the lower dimension in such a manner that the distance between neighborhood points is preserved. So, t-SNE preserves the local structure of the data as well.
I’ll try to explain how it does what it does.
For a given point in n-dimensional hyperspace, it calculates the distance of that point from all the other points and converts these distributions of distances to <a href="https://en.wikipedia.org/wiki/Student%27s_t-distribution">student’s t-distribution</a>. This is done for all the points such that in the end, each point has its own t-distribution of distances from all the other points.
Now the points are randomly scattered in the lower dimensional space and each point is displaced by some distance such that after the displacement of all the points is done if we recalculate the t-distribution of distances of each point from the remaining points (this time this is done in the lower dimensional space), the distribution would be the same as what we obtained in n-dimensional hyperspace.
There are 2 main hyperparameters in t-SNE-
<strong><em>Perplexity:</em></strong> Instead of calculating the distance from all the other points, we can use only ‘k’ nearest points. This value of ‘k’ is called the perplexity value.
<strong><em>Iterations:</em></strong> The number of iterations for which we want t-SNE to update the points in lower-dimensional space.
Due to stochasticity, the algorithm may perform differently for different perplexity values so as a good practice, it is preferred to run t-SNE for different perplexity values and different numbers of iterations.
To know more about t-SNE, check out <a href="https://distill.pub/2016/misread-tsne/">this awesome blog</a>, it has t-SNE very well explained with interactive visualization.</p>

<p>Below is the plot using t-SNE in two dimensions for different perplexity and iteration values. We can see t-SNE working well with perplexity 20 and 2000 iterations.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td class="code"><pre><span class="c1"># importing necessary libraries
</span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm_notebook</span>
<span class="c1"># initializing 16 subplots
</span><span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">20</span><span class="p">))</span>
<span class="c1"># transorforming the data to lower dimensions using TruncatedSVD
</span><span class="n">trans</span> <span class="o">=</span> <span class="n">TruncatedSVD</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">svd_dim</span> <span class="o">=</span> <span class="n">trans</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">words_tfidf</span><span class="p">.</span><span class="n">toarray</span><span class="p">())</span>
<span class="c1"># iterating over different perplexity and iteration values and plotting.
</span><span class="n">perplexity</span> <span class="o">=</span> <span class="p">[</span><span class="mi">10</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">60</span><span class="p">]</span>
<span class="n">iterations</span> <span class="o">=</span> <span class="p">[</span><span class="mi">500</span><span class="p">,</span><span class="mi">1000</span><span class="p">,</span><span class="mi">1500</span><span class="p">,</span><span class="mi">2000</span><span class="p">]</span>
<span class="n">p_</span><span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">tqdm_notebook</span><span class="p">(</span><span class="n">perplexity</span><span class="p">):</span>
  <span class="n">i_</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm_notebook</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">trans_</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">perplexity</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">node_embeddings_2d</span> <span class="o">=</span> <span class="n">trans_</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">svd_dim</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">p_</span><span class="p">,</span><span class="n">i_</span><span class="p">].</span><span class="n">scatter</span><span class="p">(</span><span class="n">node_embeddings_2d</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span>
                      <span class="n">node_embeddings_2d</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span>
                      <span class="n">c</span><span class="o">=</span><span class="n">clf</span><span class="p">.</span><span class="n">clusters</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">p_</span><span class="p">,</span> <span class="n">i_</span><span class="p">].</span><span class="n">set_title</span><span class="p">(</span><span class="sa">f</span><span class="s">'perplexity:</span><span class="si">{</span><span class="n">p</span><span class="si">}</span><span class="s">; iterations:</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">i_</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
      <span class="n">ax</span><span class="p">[</span><span class="n">p_</span><span class="p">,</span> <span class="n">i_</span><span class="p">].</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'1st dimension'</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">p_</span><span class="o">==</span><span class="mi">3</span><span class="p">:</span>
      <span class="n">ax</span><span class="p">[</span><span class="n">p_</span><span class="p">,</span> <span class="n">i_</span><span class="p">].</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'2nd dimension'</span><span class="p">)</span>
    <span class="n">i_</span><span class="o">+=</span><span class="mi">1</span>
  <span class="n">p_</span><span class="o">+=</span><span class="mi">1</span>
<span class="n">plt</span><span class="p">.</span><span class="n">savefig</span><span class="p">(</span><span class="s">'tsne_plot.png'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2880/1*RQ1ZtC9P8ZhcQPbHsUhsEg.png" />
    </div>
</div>

<p>Finally, Here is the complete graph of all the words and their synonyms (I used 4 synonyms for each word) using networkx.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="code"><pre><span class="c1"># importing the necessary libraries
</span><span class="kn">import</span> <span class="nn">networkx</span> <span class="k">as</span> <span class="n">nx</span>
<span class="kn">from</span> <span class="nn">networkx.algorithms</span> <span class="kn">import</span> <span class="n">bipartite</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="c1"># importing the data
</span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'barrons333_4.csv'</span><span class="p">)</span> <span class="c1"># contains word-synonym pair
# The below list contains word-synonym pair as tuples
</span><span class="n">edges</span> <span class="o">=</span> <span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">tolist</span><span class="p">()]</span>
<span class="c1"># defining graph object, adding words as nodes and links to synonyms
</span><span class="n">B</span> <span class="o">=</span> <span class="n">nx</span><span class="p">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="n">B</span><span class="p">.</span><span class="n">add_nodes_from</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'word'</span><span class="p">].</span><span class="n">unique</span><span class="p">(),</span> <span class="n">bipartite</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'word'</span><span class="p">)</span>
<span class="n">B</span><span class="p">.</span><span class="n">add_edges_from</span><span class="p">(</span><span class="n">edges</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'links'</span><span class="p">)</span>
<span class="c1"># defining the type of layout for the graph
</span><span class="n">pos_</span> <span class="o">=</span> <span class="n">nx</span><span class="p">.</span><span class="n">spring_layout</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="c1"># using pyplot to plot the graph.
</span><span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span><span class="mi">45</span><span class="p">))</span>
<span class="n">nx</span><span class="p">.</span><span class="n">draw</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">with_labels</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">pos_</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">edge_color</span><span class="o">=</span><span class="s">'gray'</span><span class="p">,</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">word_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'word'</span><span class="p">].</span><span class="n">unique</span><span class="p">())</span>
<span class="n">nx</span><span class="p">.</span><span class="n">draw_networkx_nodes</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">pos</span><span class="o">=</span><span class="n">pos_</span><span class="p">,</span> <span class="n">nodelist</span><span class="o">=</span><span class="n">word_list</span><span class="p">,</span>
                     <span class="p">...</span> <span class="n">node_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">node_color</span><span class="o">=</span><span class="s">'g'</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
 
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/5788/1*5MjLkhDoPNcU-eVi2T-J3A.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Final note</strong></p>
</blockquote>

<p>Thank you for reading the blog. I hope it was useful for some of you aspiring to do projects on NLP, unsupervised machine-learning, data processing, data visualizing.</p>

<p>And if you have any doubts regarding this project, please leave a comment in the response section or in the GitHub repo of this project.</p>

<p>The full project is available on my Github:
<a href="https://github.com/SarthakV7/Clustering-Barron-s-333-word-list-using-unsupervised-machine-learning">https://github.com/SarthakV7/Clustering-Barron-s-333-word-list-using-unsupervised-machine-learning</a></p>

<p>Find me on LinkedIn: <a href="http://www.linkedin.com/in/sarthak-vajpayee">www.linkedin.com/in/sarthak-vajpayee</a></p>

<p>Peace! ☮</p>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Sarthak  Vajpayee.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/my_folio/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/my_folio/assets/js/common.js"></script>


</html>
