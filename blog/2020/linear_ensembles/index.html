<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Sarthak  Vajpayee | How powerful can an ensemble of linear models be?</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->
<link rel="shortcut icon" href="/my_folio/assets/img/favicon.ico">
<link rel="stylesheet" href="/my_folio/assets/css/main.css">

<link rel="canonical" href="/my_folio/blog/2020/linear_ensembles/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/my_folio/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/my_folio/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/my_folio/">
       <span class="font-weight-bold">Sarthak</span>   Vajpayee
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/my_folio/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/my_folio/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/my_folio/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/my_folio/publications/">
                publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/my_folio/teaching/">
                skills
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">How powerful can an ensemble of linear models be?</h1>
    <p class="post-meta">May 29, 2020</p>
  </header>

  <article class="post-content">
    <h4 id="how-an-ensemble-of-linear-models-got-in-the-top-6-of-mercari-price-prediction-challenge-leaderboard-on-kaggle">How an ensemble of linear models got in the top 6% of Mercari price prediction challenge leaderboard on Kaggle.</h4>

<p><em>With the rapid growth of deep learning algorithms in recent years, today they have become a state of the art in AI. And this makes me wonder if the traditional and old school machine learning techniques like Linear Regression, Support Vector Machines, etc are still decent enough that they can go head to head with deep learning techniques?
To look over the capabilities of these often overlooked machine learning techniques I will be solving a Kaggle competition problem using only traditional machine learning techniques (no neural networks).</em></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3952/1*QLU58lo_X3qE_HLb70MTmg.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Note: I’ll be using python 3.7 for this project.</strong></p>
</blockquote>

<h3 id="birds-eye-view-of-the-blog-">Bird’s eye view of the blog-</h3>

<p>The project is divided into 6 major steps-</p>

<ul>
  <li>
    <p>Business problem and evaluation metrics</p>
  </li>
  <li>
    <p>About the data</p>
  </li>
  <li>
    <p>Exploratory Data Analysis</p>
  </li>
  <li>
    <p>Data preprocessing</p>
  </li>
  <li>
    <p>Modeling</p>
  </li>
  <li>
    <p>Obtaining scores from Kaggle leaderboard.</p>
  </li>
</ul>

<p><strong><em>Business problem and Evaluation metrics</em></strong></p>

<p>It can be hard to know how much something’s really worth. Small details can mean big differences in pricing. For example, one of these sweaters cost $335 and the other cost $9.99. Can you guess which one’s which?</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2808/1*KR9rv6UCu2XR90ldot_R0g.png" />
    </div>
</div>

<p><img src="" alt="" />
Product pricing gets even harder at scale, considering just how many products are sold online. Clothing has strong seasonal pricing trends and is heavily influenced by brand names, while electronics have fluctuating prices based on product specifications.
<a href="https://www.mercari.com/">Mercari</a>, Japan’s biggest community-powered shopping app, knows this problem deeply. They’d like to offer pricing suggestions to sellers, but this is tough because their sellers are enabled to put just about anything, or any bundle of things, on Mercari’s marketplace.
In this competition, we need to build an algorithm that automatically suggests the right product prices. We’ll be provided with text descriptions of products, and features including details like product category name, brand name, and item condition.</p>

<p>The evaluation metric for this competition is <a href="https://www.kaggle.com/wiki/RootMeanSquaredLogarithmicError">Root Mean Squared Logarithmic Error</a>. The RMSLE is calculated as:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2820/1*tZY-2x2IdDbbBoVIKxwQHA.png" />
    </div>
</div>

<p><img src="" alt="" />
Where:
<em>ϵ</em> is the RMSLE value (score)
<em>n</em> is the total number of observations in the (public/private) data set,
<em>pi</em> is the prediction of price,
<em>ai</em> is the actual sale price for <em>i</em>.
<em>log(x)</em> is the natural logarithm of x</p>

<p><strong><em>About the data</em></strong></p>

<p>The data we’ll be using is provided by Mercari and can be found on Kaggle using <a href="https://www.kaggle.com/c/mercari-price-suggestion-challenge/data">this</a> link. The data lists details about products from the Mercari website.
Let’s check out one of the products from the website and how it is described in the dataset.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4920/1*F_qskp-MUrFcMys310zPng.jpeg" />
    </div>
</div>

<p><img src="" alt="" />
<strong>The dataset has 8 features:</strong></p>

<ul>
  <li>
    <p><strong>Train_id/Test_id:</strong> Every item in the dataset has a unique item id. This will be used while submitting the predicted prices.</p>
  </li>
  <li>
    <p><strong>Name:</strong> Represents the name of the product, it is in string format. For the above product, the name is <em>‘Ayn Rand The Fountainhead’</em></p>
  </li>
  <li>
    <p><strong>Item condition:</strong> A number provided by the seller that denotes the condition of the item. It can take a value between 1 and 5. In our case, the condition of the product is ‘*good’ *so it’ll be denoted by 4 in the dataset.</p>
  </li>
  <li>
    <p><strong>Category name:</strong> Represents the category of the item. For the above item, the category mentioned in the dataset is *‘other/books/Literature &amp; Fiction’
*and this feature is also of datatype string.</p>
  </li>
  <li>
    <p><strong>Brand name:</strong> Represents the name of the brand the item belongs to. For the above product, the brand-name is <em>‘Penguin Random House’</em>.</p>
  </li>
  <li>
    <p><strong>Price:</strong> Represents the price of the item, in our case, this will be the target value that we need to predict. The unit is USD. For the above product, the price provided is <em>‘$9’</em>.</p>
  </li>
  <li>
    <p><strong>Shipping:</strong> A number that represents the type of shipping available on the product. Shipping will be 1 if the shipping fee is paid by the seller and 0 if the fee is paid by the buyer. For the above product, the shipping is free so in the dataset, this feature will be 1.</p>
  </li>
  <li>
    <p><strong>Item description:</strong> The full description of the item. For the above product, the description says, *“The Fountainhead” pocket paperback book — by Ayn Rand — Centennial Edition — classic literature — Book is in good condition with some wear on covers and corners (see pictures)”. *This feature comes already in a preprocessed form in the provided dataset.</p>
  </li>
</ul>

<p>Let’s import the data using pandas and check the first 5 entries.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'train.tsv'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">)</span>
    <span class="n">df_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'test.tsv'</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">'</span><span class="se">\t</span><span class="s">'</span><span class="p">)</span>
    <span class="n">data</span><span class="p">.</span><span class="n">head</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4284/1*plSzaXxtFbxTSFD5LHTuhQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<p><strong><em>Exploratory Data Analysis (EDA)</em></strong></p>

<p>In this section, we’ll be exploring and analyzing the data in depth. We’ll be covering the data feature by feature.</p>
<blockquote>
  <p><strong>Price</strong></p>
</blockquote>

<p>This is the target feature that we need to predict using the information about the product in the form of other features.
Let’s check out the statistical summary of this feature using describe()</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="s">'price'</span><span class="p">].</span><span class="n">describe</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*P3GA8wWo1S1INdBR6q6Y4w.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>There are about 1.48 million products in the dataset. The costliest product is priced at $ 2009, the cheapest product is priced at $ 3 whereas the mean price is $ 26.75</li>
</ul>

<p>Now we’ll take a look at the histogram of the prices. Here, I’ve used the number of bins as 200.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'price'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'price'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'frequency'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'histogram of price'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2036/1*6U-7smRLqG654I2EchjKCw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>We can observe that the distribution follows a power-law distribution, to fix that, and to make it kind of Gaussian distribution, let’s convert the values to the log form i.e. we’ll be replacing the price values with log(price+1).</li>
</ul>

<p>We are converting the prices to Normal distribution as it is one of the most well-known distributions in statistics because it fits many natural phenomena and this makes it one of the most easily interpretable distributions that we can do analysis on. Another reason for transforming the data into a normal distribution is that the variance in price is reduced and most of the points are centered around the mean which makes the price prediction much easier for the model.</p>

<p>I’ve already converted the data into a log form. Here is the histogram of the log(price+1).</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'price_log'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'log(price + 1)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'frequency'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'histogram of log of price'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*VKo8apbiIIDD5jnnDdcung.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can observe that the distribution is much more interpretable now and tries to follow a Normal distribution.</p>
  </li>
  <li>
    <p>Also, notice how most of the points are centered around the mean (the mean is somewhere near 3).</p>
    <blockquote>
      <p><strong>item_condition_id</strong></p>
    </blockquote>
  </li>
</ul>

<p>This is a categorical feature that denotes the condition of the item. Let’s check out more about it using value_counts()</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="s">'item_condition_id'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*od3gkug_DbF-c0qDO3-mJg.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>The output tells us that this feature can take up 5 values between 1 and 5, and the number of items with that particular condition is mentioned next to it.</li>
</ul>

<p>Let’s look at the bar-graph of this feature</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre>    <span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'item_condition_id'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">().</span><span class="n">keys</span><span class="p">(),</span>
                <span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'item_condition_id'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">())</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'item condition type'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'number of products'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'bar graph of "item condition type"'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*MQqGF0Uqj4rkEMAPez6yUA.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>We can see that a majority of items have a condition id of 1, and only very few items have a condition id of 5.</li>
</ul>

<p>Now let’s compare the price distribution of products with different item_condition_id</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3328/1*AOpvTg4KdoiYUkMpLVPQjA.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2432/1*6uTtP9eKjbiS4vQmqaKxcA.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>We can see that the price distributions of items having different item_condition_id are very similar.</li>
</ul>

<p>Let’s check out the boxplot and violin plot of the price distribution of products with different item_condition_id.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre>    <span class="c1"># plotting box-plot
</span>    <span class="n">sns</span><span class="p">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'item_condition_id'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'price_log'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

    <span class="c1"># plotting violin plot
</span>    <span class="n">sns</span><span class="p">.</span><span class="n">violinplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'item_condition_id'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'price_log'</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2752/1*QLHsX6AoirpCRaUy3Hqnjw.png" />
    </div>
</div>

<p><img src="" alt="" />
The boxplot and violin plots also tell us that the price distributions of items with different item_condition_id are not so different, also the distributions are a bit right-skewed. Products with item_condition_id = 5 have the highest median price whereas products with item_condition_id = 4 have the lowest median price. Most of the products have a price in the range of 1.5 and 5.2</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Category name</strong></p>
</blockquote>

<p>This is a text type data that tells us about the category of the product.
Let’s check out the statistical summary of the feature category name-</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="s">'category_name'</span><span class="p">].</span><span class="n">describe</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2036/1*jfUM2Vybwzar6wGW3QFADw.png" />
    </div>
</div>

<p><img src="" alt="" />
These are string type features that are actually 3 sub-categories joined into 1.
Let’s consider the most frequently occurring category name feature ‘Women/Athletic Apparel/Pants, Tights, Leggings’ as mentioned in the above description. It can be broken down into 3 sub-categories:</p>
<ul>
  <li>sub-category_1: ‘Women’</li>
  <li>sub-category_2: ‘Athletic Apparel’</li>
  <li>sub-category_3: ‘Pants, Tights, Leggings’
To make the visualization for this feature easy, I’ll consider this feature sub-category wise. Let’s divided the data sub-category wise.</li>
</ul>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre>    <span class="c1"># this is to divide the category_name feature into 3 sub categories
</span>    <span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm_notebook</span>
    <span class="n">sub_category_1</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">sub_category_2</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">sub_category_3</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">tqdm_notebook</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'category_name'</span><span class="p">].</span><span class="n">values</span><span class="p">):</span>
      <span class="n">fs</span> <span class="o">=</span> <span class="n">feature</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">'/'</span><span class="p">)</span>
      <span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">c</span> <span class="o">=</span> <span class="n">fs</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">fs</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">fs</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>
      <span class="n">sub_category_1</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
      <span class="n">sub_category_2</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>
      <span class="n">sub_category_3</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>

    <span class="n">data</span><span class="p">[</span><span class="s">'sub_category_1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_category_1</span>
    <span class="n">data</span><span class="p">[</span><span class="s">'sub_category_2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_category_2</span>
    <span class="n">data</span><span class="p">[</span><span class="s">'sub_category_3'</span><span class="p">]</span> <span class="o">=</span> <span class="n">sub_category_3</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Sub-category_1</strong></p>
</blockquote>

<p>Let’s check the statistical description:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="s">'sub_category_1'</span><span class="p">].</span><span class="n">describe</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*CWomypnKmI1peXK8JMfQaQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>There are around 1.4M of these in our data, that can take 11 distinct values. The most frequent of these are Women.</li>
</ul>

<p>Let’s plot the bar graph of sub-category 1</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre>    <span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'sub_category_1'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">().</span><span class="n">keys</span><span class="p">(),</span> <span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'sub_category_1'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">())</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'number of products'</span><span class="p">)</span>
    <span class="n">locs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">setp</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'bar-plot of sub_category_1'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*-A2A0l68XqYFIKf0dlVt_A.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can see that most of the items have sub_category_1 as ‘women’ and the least items have ‘Sports &amp; Outdoors’.</p>
  </li>
  <li>
    <p>Note that items with no sub_category_1 defined are denoted with ‘no label’.</p>
  </li>
</ul>

<p>Let’s check the distribution of sub_category_1 and log of price</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">sns</span><span class="p">.</span><span class="n">FacetGrid</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s">"sub_category_1"</span><span class="p">,</span> <span class="n">height</span><span class="o">=</span><span class="mi">5</span><span class="p">).</span><span class="nb">map</span><span class="p">(</span><span class="n">sns</span><span class="p">.</span><span class="n">distplot</span><span class="p">,</span> <span class="s">'price_log'</span><span class="p">).</span><span class="n">add_legend</span><span class="p">();</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'comparing the log of price distribution of products with
               sub_category_1</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'PDF of log of price'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2404/1*2vj-JoEqTsgORuijWVnSkg.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can see that most of the distributions are right-skewed with a little difference.</p>
  </li>
  <li>
    <p>The sub-category ‘handmade’ is slightly distinguishable as we can see some products in this category with log(price) of less than 2</p>
  </li>
</ul>

<p>Now let’s take a look at the violin plots of sub_category_1</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2748/1*d7kLtI9HxuPd1h5G6mgqTA.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>Looking at the violin plot, we can say that the distribution of items with ‘men’ as sub_category_1 tends to be on the pricier end whereas items with ‘handmade’ as sub_category_1 tend to be on the economical end.</li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Sub_category_2</strong></p>
</blockquote>

<p>Let’s check the statistical description of sub_category_2:</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="s">'sub_category_2'</span><span class="p">].</span><span class="n">describe</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*GHKDffRHGzbx6gyGbyl0CQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>sub_category_2 has 114 distinct values, let’s analyze the top 20 categories of sub_category_2.</li>
</ul>

<p>Bar graph of the top 20 categories in sub_category_2</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'sub_category_2'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">().</span><span class="n">keys</span><span class="p">()[:</span><span class="mi">20</span><span class="p">],</span>
                <span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'sub_category_2'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">()[:</span><span class="mi">20</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'number of products'</span><span class="p">)</span>
    <span class="n">locs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">setp</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">90</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'bar-plot of top 20 sub_category_2'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2656/1*-WiCIllod2FpJj2z-QV88Q.png" />
    </div>
</div>

<ul>
  <li>We can see that most of the items have sub_category_2 as ‘authentic apparel’ followed by ‘Makeup’ and then ‘Tops &amp; Blouses’.</li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Sub_category_3</strong></p>
</blockquote>

<p>Let’s check the statistical description of sub_category_3:</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*G1wl1JPJgrjE1ZldJaKwRQ.png" />
    </div>
</div>

<ul>
  <li>sub_category_3 has 865 distinct values, let’s analyze the histogram of the top 20 categories of sub_category_3.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2608/1*eJAARMyWMKv7mNEky4jW0A.png" />
    </div>
</div>

<ul>
  <li>We can see that most of the items have sub_category_3 as ‘Pants, Tights, Leggings’ followed by ‘Other’ and ‘Face’.</li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Brand name</strong></p>
</blockquote>

<p>This is another text type feature that denotes the brand the product belongs to. Let’s check out the statistical summary of the feature brand_name.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*xts6ErQ_Ke0IivDCUHOThw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>Here, we can see that there are a total of 4089 distinct brand names.</li>
</ul>

<p>Let’s check the histogram of the top 20 brands</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'brand_name'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">().</span><span class="n">keys</span><span class="p">()[:</span><span class="mi">20</span><span class="p">],</span>
                <span class="n">y</span><span class="o">=</span><span class="n">data</span><span class="p">[</span><span class="s">'brand_name'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">()[:</span><span class="mi">20</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'number of products'</span><span class="p">)</span>
    <span class="n">locs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">setp</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'bar-plot of top 20 brands (including products with
               unknown brand)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2656/1*lciLuLRNEM_kynJkkLsIOw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>Note that here, ‘unknown’ represents the item with no brand specified.</p>
  </li>
  <li>
    <p>PINK, Nike, and Victoria’s Secret are the top 3 brands with most items on the website.</p>
  </li>
</ul>

<p>Let’s see the bar-plot of the top 20 brands with their mean product price.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'brand_name'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">20</span><span class="p">],</span>
                <span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s">'price'</span><span class="p">].</span><span class="n">values</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'average price of products'</span><span class="p">)</span>
    <span class="n">locs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">xticks</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">setp</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'bar-plot of top 20 brands with their mean product price'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2560/1*XkUZpI9qlRiH_WBCvAjFGw.png" />
    </div>
</div>

<p>Let’s see the bar-plot of the top 20 brands with maximum product price</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2544/1*MxZyU5j5VpiG_sLch804KQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Shipping</strong></p>
</blockquote>

<p>This is a numerical categorical data type that can take 2 values, 0s or 1s
Let’s check out its statistical description.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="s">'shipping'</span><span class="p">].</span><span class="n">value_counts</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*cEWev-hX6tiBJfhvDP4kJQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>There are about 22% more items with shipping as 0 than 1.</li>
</ul>

<p>Let’s compare the log of price distribution of products with different shipping.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*SfOFnHY5SpS6jhiWBhTTPQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can see that the log of price distribution of items with different shipping has a slight variance.</p>
  </li>
  <li>
    <p>The products with shipping as 1 tend to have a lower price.</p>
  </li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>item_description (text)</strong></p>
</blockquote>

<p>This is a text type feature that describes the product. Let’s take a look at some of these.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="s">'item_description'</span><span class="p">]</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*Ho4OUDAoOby4WAyanoUqXQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>We can see that there are a total of 1482535 of these.</li>
</ul>

<p>We’ll be using this feature as is after performing some NLP techniques which will be discussed later in this blog.
Another thing that we can do with this feature is, calculate it’s word-length i.e. the number of words this feature contains for each product and do some analysis on that.
Let’s check the statistical summary of the word_length of the item description.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">data</span><span class="p">[</span><span class="s">'item_description_word_length'</span><span class="p">].</span><span class="n">describe</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*YKmRGP0MxxMows0yZyQhKA.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>We can see that the longest description has 245 words and the shortest has no words. On average the words are around 25</li>
</ul>

<p>Let’s plot the histogram of item_description_word_length,</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'item_description_word_length'</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'item_description_word_length'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'frequency'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'histogram of item_description_word_length'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*fRm22uuLM42OFIh5iSz5ng.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can see that the histogram of word length follows a power-law distribution.</p>
  </li>
  <li>
    <p>I’ve used 200 bins for this histogram.</p>
  </li>
</ul>

<p>Let’s try to convert this into a Normal distribution by taking the log of the word length. Here is what the distribution looks like.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'log_item_description_word_length'</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'log(item_description_word_length + 1)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'frequency'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'histogram of log of item_description_word_length'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*zBS-98suZ_EvyBD7yuzqJw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can see that this feature tries to follow a Normal distribution.</p>
  </li>
  <li>
    <p>Most of the items have a word length between 5 and 20. (values obtained from antilog).</p>
  </li>
  <li>
    <p>We can use this as a feature for modeling.</p>
  </li>
</ul>

<p>Now let’s see how the log(item_word_length) affects the price of the item</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*9q81XPQaqbcThxuiMpna3Q.png" />
    </div>
</div>

<ul>
  <li>
    <p>We can see that the log of price increases as the item_word_length goes from 0 to 50 but then the prices tend to come down except the spike that we can observe near word length of around 190.</p>
  </li>
  <li>
    <p>Also, the prices are much more volatile for word length more than 100.</p>
  </li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Name of the product</strong></p>
</blockquote>

<p>Finally, let’s check out the last feature that is the name of the product. This is also a text type feature and we’ll be performing NLP on it later but first, let’s do some analysis on it by plotting the histogram of the number of words in the ‘name’ feature.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s">'name_length'</span><span class="p">])</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'name_length'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'frequency'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'histogram of name_length'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*a6FJNlDpETi4rWfojTozrQ.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>The distribution is visibly left-skewed and maximum items have a name length of about 25.</li>
</ul>

<p>Let’s see how the prices vary with the number of words in the product’s name.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>    <span class="n">df</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'name_length'</span><span class="p">)[</span><span class="s">'price_log'</span><span class="p">].</span><span class="n">mean</span><span class="p">().</span><span class="n">reset_index</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">relplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">"name_length"</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">"price_log"</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">"line"</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*IfkJ_h-YkQIsjapC9RyWMA.png" />
    </div>
</div>

<ul>
  <li>
    <p>Note that I’m using the log of prices instead of the actual prices.</p>
  </li>
  <li>
    <p>We can see that the distribution is much linear for name_length values between 10 and 38 and then there’s a sharp drop and rise.</p>
  </li>
</ul>

<p><img src="" alt="" />
<strong><em>Data preprocessing</em></strong></p>

<p>In this step, we’ll be cleaning the data and make it ready for modeling.
Remember that we have 6 features out of which, we have:</p>
<ul>
  <li>4 text features: Name, description, brand name, and category</li>
  <li>2 categorical features: shipping and the item_condition_id</li>
</ul>

<p>Let’s start by cleaning the text features and for that, we’ll define some functions-</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre></td><td class="code"><pre>  <span class="kn">import</span> <span class="nn">re</span>
  <span class="k">def</span> <span class="nf">decontracted</span><span class="p">(</span><span class="n">phrase</span><span class="p">):</span>
      <span class="c1"># specific
</span>      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"won't"</span><span class="p">,</span> <span class="s">"will not"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"can\'t"</span><span class="p">,</span> <span class="s">"can not"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="c1"># general
</span>      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"n\'t"</span><span class="p">,</span> <span class="s">"not"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'re"</span><span class="p">,</span> <span class="s">" are"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'s"</span><span class="p">,</span> <span class="s">" is"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'d"</span><span class="p">,</span> <span class="s">" would"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'ll"</span><span class="p">,</span> <span class="s">" will"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'t"</span><span class="p">,</span> <span class="s">" not"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'ve"</span><span class="p">,</span> <span class="s">" have"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="n">phrase</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="sa">r</span><span class="s">"\'m"</span><span class="p">,</span> <span class="s">" am"</span><span class="p">,</span> <span class="n">phrase</span><span class="p">)</span>
      <span class="k">return</span> <span class="n">phrase</span>
  
</pre></td></tr></tbody></table></code></pre></figure>

<p>The function works by decontracting words like “we’ll” to “we will”, “can’t” to “cannot”, “we’re” to “we are” etc. This step is necessary because we do not want our model to treat phrases like “we’re” and “we are” differently.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
</pre></td><td class="code"><pre>    <span class="n">stopwords</span><span class="o">=</span> <span class="p">[</span><span class="s">'i'</span><span class="p">,</span> <span class="s">'me'</span><span class="p">,</span> <span class="s">'my'</span><span class="p">,</span> <span class="s">'myself'</span><span class="p">,</span> <span class="s">'we'</span><span class="p">,</span> <span class="s">'our'</span><span class="p">,</span> <span class="s">'ours'</span><span class="p">,</span> <span class="s">'ourselves'</span><span class="p">,</span> <span class="s">'you'</span><span class="p">,</span> <span class="s">"you're"</span><span class="p">,</span> <span class="s">"you've"</span><span class="p">,</span><span class="s">"you'll"</span><span class="p">,</span> <span class="s">"you'd"</span><span class="p">,</span> <span class="s">'your'</span><span class="p">,</span> <span class="s">'yours'</span><span class="p">,</span> <span class="s">'yourself'</span><span class="p">,</span> <span class="s">'yourselves'</span><span class="p">,</span> <span class="s">'he'</span><span class="p">,</span> <span class="s">'him'</span><span class="p">,</span> <span class="s">'his'</span><span class="p">,</span> <span class="s">'himself'</span><span class="p">,</span> <span class="s">'she'</span><span class="p">,</span> <span class="s">"she's"</span><span class="p">,</span> <span class="s">'her'</span><span class="p">,</span> <span class="s">'hers'</span><span class="p">,</span> <span class="s">'herself'</span><span class="p">,</span> <span class="s">'it'</span><span class="p">,</span> <span class="s">"it's"</span><span class="p">,</span> <span class="s">'its'</span><span class="p">,</span> <span class="s">'itself'</span><span class="p">,</span> <span class="s">'they'</span><span class="p">,</span> <span class="s">'them'</span><span class="p">,</span> <span class="s">'their'</span><span class="p">,</span><span class="s">'theirs'</span><span class="p">,</span> <span class="s">'themselves'</span><span class="p">,</span> <span class="s">'what'</span><span class="p">,</span> <span class="s">'which'</span><span class="p">,</span> <span class="s">'who'</span><span class="p">,</span> <span class="s">'whom'</span><span class="p">,</span> <span class="s">'this'</span><span class="p">,</span> <span class="s">'that'</span><span class="p">,</span> <span class="s">"that'll"</span><span class="p">,</span> <span class="s">'these'</span><span class="p">,</span> <span class="s">'those'</span><span class="p">,</span> <span class="s">'am'</span><span class="p">,</span> <span class="s">'is'</span><span class="p">,</span> <span class="s">'are'</span><span class="p">,</span> <span class="s">'was'</span><span class="p">,</span> <span class="s">'were'</span><span class="p">,</span> <span class="s">'be'</span><span class="p">,</span> <span class="s">'been'</span><span class="p">,</span> <span class="s">'being'</span><span class="p">,</span> <span class="s">'have'</span><span class="p">,</span> <span class="s">'has'</span><span class="p">,</span> <span class="s">'had'</span><span class="p">,</span> <span class="s">'having'</span><span class="p">,</span> <span class="s">'do'</span><span class="p">,</span> <span class="s">'does'</span><span class="p">,</span> <span class="s">'did'</span><span class="p">,</span> <span class="s">'doing'</span><span class="p">,</span> <span class="s">'a'</span><span class="p">,</span> <span class="s">'an'</span><span class="p">,</span> <span class="s">'the'</span><span class="p">,</span> <span class="s">'and'</span><span class="p">,</span> <span class="s">'but'</span><span class="p">,</span> <span class="s">'if'</span><span class="p">,</span> <span class="s">'or'</span><span class="p">,</span> <span class="s">'because'</span><span class="p">,</span> <span class="s">'as'</span><span class="p">,</span> <span class="s">'until'</span><span class="p">,</span> <span class="s">'while'</span><span class="p">,</span> <span class="s">'of'</span><span class="p">,</span> <span class="s">'at'</span><span class="p">,</span> <span class="s">'by'</span><span class="p">,</span> <span class="s">'for'</span><span class="p">,</span> <span class="s">'with'</span><span class="p">,</span> <span class="s">'about'</span><span class="p">,</span> <span class="s">'against'</span><span class="p">,</span> <span class="s">'between'</span><span class="p">,</span> <span class="s">'into'</span><span class="p">,</span> <span class="s">'through'</span><span class="p">,</span> <span class="s">'during'</span><span class="p">,</span> <span class="s">'before'</span><span class="p">,</span> <span class="s">'after'</span><span class="p">,</span><span class="s">'above'</span><span class="p">,</span> <span class="s">'below'</span><span class="p">,</span> <span class="s">'to'</span><span class="p">,</span> <span class="s">'from'</span><span class="p">,</span> <span class="s">'up'</span><span class="p">,</span> <span class="s">'down'</span><span class="p">,</span> <span class="s">'in'</span><span class="p">,</span><span class="s">'out'</span><span class="p">,</span><span class="s">'on'</span><span class="p">,</span><span class="s">'off'</span><span class="p">,</span> <span class="s">'over'</span><span class="p">,</span> <span class="s">'under'</span><span class="p">,</span> <span class="s">'again'</span><span class="p">,</span> <span class="s">'further'</span><span class="p">,</span><span class="s">'then'</span><span class="p">,</span> <span class="s">'once'</span><span class="p">,</span> <span class="s">'here'</span><span class="p">,</span> <span class="s">'there'</span><span class="p">,</span> <span class="s">'when'</span><span class="p">,</span> <span class="s">'where'</span><span class="p">,</span> <span class="s">'why'</span><span class="p">,</span><span class="s">'how'</span><span class="p">,</span><span class="s">'all'</span><span class="p">,</span> <span class="s">'any'</span><span class="p">,</span> <span class="s">'both'</span><span class="p">,</span> <span class="s">'each'</span><span class="p">,</span> <span class="s">'few'</span><span class="p">,</span> <span class="s">'more'</span><span class="p">,</span><span class="s">'most'</span><span class="p">,</span> <span class="s">'other'</span><span class="p">,</span> <span class="s">'some'</span><span class="p">,</span> <span class="s">'such'</span><span class="p">,</span> <span class="s">'only'</span><span class="p">,</span> <span class="s">'own'</span><span class="p">,</span> <span class="s">'same'</span><span class="p">,</span> <span class="s">'so'</span><span class="p">,</span><span class="s">'than'</span><span class="p">,</span> <span class="s">'too'</span><span class="p">,</span> <span class="s">'very'</span><span class="p">,</span> <span class="s">'s'</span><span class="p">,</span> <span class="s">'t'</span><span class="p">,</span> <span class="s">'can'</span><span class="p">,</span> <span class="s">'will'</span><span class="p">,</span> <span class="s">'just'</span><span class="p">,</span><span class="s">'don'</span><span class="p">,</span><span class="s">"don't"</span><span class="p">,</span><span class="s">'should'</span><span class="p">,</span><span class="s">"should've"</span><span class="p">,</span> <span class="s">'now'</span><span class="p">,</span> <span class="s">'d'</span><span class="p">,</span> <span class="s">'ll'</span><span class="p">,</span> <span class="s">'m'</span><span class="p">,</span> <span class="s">'o'</span><span class="p">,</span><span class="s">'re'</span><span class="p">,</span><span class="s">'ve'</span><span class="p">,</span><span class="s">'y'</span><span class="p">,</span><span class="s">'ain'</span><span class="p">,</span><span class="s">'aren'</span><span class="p">,</span><span class="s">"aren't"</span><span class="p">,</span><span class="s">'couldn'</span><span class="p">,</span><span class="s">"couldn't"</span><span class="p">,</span><span class="s">'didn'</span><span class="p">,</span><span class="s">"didn't"</span><span class="p">,</span> <span class="s">'doesn'</span><span class="p">,</span> <span class="s">"doesn't"</span><span class="p">,</span> <span class="s">'hadn'</span><span class="p">,</span><span class="s">"hadn't"</span><span class="p">,</span> <span class="s">'hasn'</span><span class="p">,</span> <span class="s">"hasn't"</span><span class="p">,</span> <span class="s">'haven'</span><span class="p">,</span> <span class="s">"haven't"</span><span class="p">,</span> <span class="s">'isn'</span><span class="p">,</span> <span class="s">"isn't"</span><span class="p">,</span><span class="s">'ma'</span><span class="p">,</span> <span class="s">'mightn'</span><span class="p">,</span> <span class="s">"mightn't"</span><span class="p">,</span> <span class="s">'mustn'</span><span class="p">,</span><span class="s">"mustn't"</span><span class="p">,</span> <span class="s">'needn'</span><span class="p">,</span> <span class="s">"needn't"</span><span class="p">,</span><span class="s">'shan'</span><span class="p">,</span><span class="s">"shan't"</span><span class="p">,</span><span class="s">'shouldn'</span><span class="p">,</span><span class="s">"shouldn't"</span><span class="p">,</span> <span class="s">'wasn'</span><span class="p">,</span> <span class="s">"wasn't"</span><span class="p">,</span> <span class="s">'weren'</span><span class="p">,</span> <span class="s">"weren't"</span><span class="p">,</span> <span class="s">'won'</span><span class="p">,</span> <span class="s">"won't"</span><span class="p">,</span> <span class="s">'wouldn'</span><span class="p">,</span> <span class="s">"wouldn't"</span><span class="p">,</span> <span class="s">'•'</span><span class="p">,</span> <span class="s">'❤'</span><span class="p">,</span> <span class="s">'✨'</span><span class="p">,</span> <span class="s">'$'</span><span class="p">,</span> <span class="s">'❌'</span><span class="p">,</span><span class="s">'♡'</span><span class="p">,</span> <span class="s">'☆'</span><span class="p">,</span> <span class="s">'✔'</span><span class="p">,</span> <span class="s">'⭐'</span><span class="p">,</span><span class="s">'✅'</span><span class="p">,</span> <span class="s">'⚡'</span><span class="p">,</span> <span class="s">'‼'</span><span class="p">,</span> <span class="s">'—'</span><span class="p">,</span> <span class="s">'▪'</span><span class="p">,</span> <span class="s">'❗'</span><span class="p">,</span> <span class="s">'■'</span><span class="p">,</span> <span class="s">'●'</span><span class="p">,</span> <span class="s">'➡'</span><span class="p">,</span><span class="s">'⛔'</span><span class="p">,</span> <span class="s">'♦'</span><span class="p">,</span> <span class="s">'〰'</span><span class="p">,</span> <span class="s">'×'</span><span class="p">,</span> <span class="s">'⚠'</span><span class="p">,</span> <span class="s">'°'</span><span class="p">,</span> <span class="s">'♥'</span><span class="p">,</span> <span class="s">'★'</span><span class="p">,</span> <span class="s">'®'</span><span class="p">,</span> <span class="s">'·'</span><span class="p">,</span><span class="s">'☺'</span><span class="p">,</span><span class="s">'–'</span><span class="p">,</span><span class="s">'➖'</span><span class="p">,</span><span class="s">'✴'</span><span class="p">,</span> <span class="s">'❣'</span><span class="p">,</span> <span class="s">'⚫'</span><span class="p">,</span> <span class="s">'✳'</span><span class="p">,</span> <span class="s">'➕'</span><span class="p">,</span> <span class="s">'™'</span><span class="p">,</span> <span class="s">'ᴇ'</span><span class="p">,</span> <span class="s">'》'</span><span class="p">,</span> <span class="s">'✖'</span><span class="p">,</span> <span class="s">'▫'</span><span class="p">,</span> <span class="s">'¤'</span><span class="p">,</span><span class="s">'⬆'</span><span class="p">,</span> <span class="s">'⃣'</span><span class="p">,</span> <span class="s">'ᴀ'</span><span class="p">,</span> <span class="s">'❇'</span><span class="p">,</span> <span class="s">'ᴏ'</span><span class="p">,</span> <span class="s">'《'</span><span class="p">,</span> <span class="s">'☞'</span><span class="p">,</span> <span class="s">'❄'</span><span class="p">,</span> <span class="s">'»'</span><span class="p">,</span> <span class="s">'ô'</span><span class="p">,</span> <span class="s">'❎'</span><span class="p">,</span> <span class="s">'ɴ'</span><span class="p">,</span> <span class="s">'⭕'</span><span class="p">,</span> <span class="s">'ᴛ'</span><span class="p">,</span><span class="s">'◇'</span><span class="p">,</span> <span class="s">'ɪ'</span><span class="p">,</span> <span class="s">'½'</span><span class="p">,</span> <span class="s">'ʀ'</span><span class="p">,</span> <span class="s">'❥'</span><span class="p">,</span> <span class="s">'⚜'</span><span class="p">,</span> <span class="s">'⋆'</span><span class="p">,</span> <span class="s">'⏺'</span><span class="p">,</span> <span class="s">'❕'</span><span class="p">,</span> <span class="s">'ꕥ'</span><span class="p">,</span> <span class="s">'：'</span><span class="p">,</span> <span class="s">'◆'</span><span class="p">,</span> <span class="s">'✽'</span><span class="p">,</span><span class="s">'…'</span><span class="p">,</span> <span class="s">'☑'</span><span class="p">,</span> <span class="s">'︎'</span><span class="p">,</span> <span class="s">'═'</span><span class="p">,</span> <span class="s">'▶'</span><span class="p">,</span> <span class="s">'⬇'</span><span class="p">,</span> <span class="s">'ʟ'</span><span class="p">,</span> <span class="s">'！'</span><span class="p">,</span> <span class="s">'✈'</span><span class="p">,</span> <span class="s">'�'</span><span class="p">,</span> <span class="s">'☀'</span><span class="p">,</span> <span class="s">'ғ'</span><span class="p">]</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>In the above code block, I’ve defined a list containing the stop words. Stop words are words that do not add much semantic or literal meaning to sentences. Most of these are contracted representations of words or not so important words like ‘a’, ‘at’, ‘for’ etc, and symbols.</p>

<p>Now we’ll define a function that takes the sentences, and uses the deconcatenated function and stopwords list to clean and return processed text.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="code"><pre>      <span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm_notebook</span>
      <span class="k">def</span> <span class="nf">preprocess_text</span><span class="p">(</span><span class="n">text_data</span><span class="p">):</span>
        <span class="n">preprocessed_text</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># tqdm is for printing the status bar
</span>        <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">tqdm_notebook</span><span class="p">(</span><span class="n">text_data</span><span class="p">):</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="n">decontracted</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'</span><span class="se">\\</span><span class="s">r'</span><span class="p">,</span> <span class="s">' '</span><span class="p">)</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'</span><span class="se">\\</span><span class="s">n'</span><span class="p">,</span> <span class="s">' '</span><span class="p">)</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="n">sent</span><span class="p">.</span><span class="n">replace</span><span class="p">(</span><span class="s">'</span><span class="se">\\</span><span class="s">"'</span><span class="p">,</span> <span class="s">' '</span><span class="p">)</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="n">re</span><span class="p">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'[^A-Za-z0-9]+'</span><span class="p">,</span> <span class="s">' '</span><span class="p">,</span> <span class="n">sent</span><span class="p">)</span>
          <span class="n">sent</span> <span class="o">=</span> <span class="s">' '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">e</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">.</span><span class="n">split</span><span class="p">()</span> <span class="k">if</span> <span class="n">e</span><span class="p">.</span><span class="n">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span>
                          <span class="n">stopwords</span><span class="p">)</span>
          <span class="n">preprocessed_text</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sent</span><span class="p">.</span><span class="n">lower</span><span class="p">().</span><span class="n">strip</span><span class="p">())</span>
        <span class="k">return</span> <span class="n">preprocessed_text</span>
      
</pre></td></tr></tbody></table></code></pre></figure>

<p>Time to clean our text data using preprocess_text() function.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre>   <span class="n">df</span><span class="p">[</span><span class="s">'name'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s">'name'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="s">''</span><span class="p">)</span> <span class="o">+</span> <span class="s">' '</span> <span class="o">+</span>
                          <span class="n">df</span><span class="p">[</span><span class="s">'brand_name'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="s">''</span><span class="p">)</span>
   <span class="n">df</span><span class="p">[</span><span class="s">'name'</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_text</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">name</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>

   <span class="n">df</span><span class="p">[</span><span class="s">'text'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s">'item_description'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="s">''</span><span class="p">)</span><span class="o">+</span>
                          <span class="s">' '</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s">'category_name'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="s">''</span><span class="p">))</span>
   <span class="n">df</span><span class="p">[</span><span class="s">'text'</span><span class="p">]</span> <span class="o">=</span> <span class="n">preprocess_text</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">text</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>

   <span class="n">df_test</span><span class="p">[</span><span class="s">'name'</span><span class="p">]</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="s">'name'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="s">''</span><span class="p">)</span> <span class="o">+</span> <span class="s">' '</span>
                     <span class="o">+</span> <span class="n">df_test</span><span class="p">[</span><span class="s">'brand_name'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="s">''</span><span class="p">)</span>
   <span class="n">df_test</span><span class="p">[</span><span class="s">'text'</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">df_test</span><span class="p">[</span><span class="s">'item_description'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="s">''</span><span class="p">)</span> <span class="o">+</span> <span class="s">' '</span>
                      <span class="o">+</span> <span class="n">df_test</span><span class="p">[</span><span class="s">'category_name'</span><span class="p">].</span><span class="n">fillna</span><span class="p">(</span><span class="s">''</span><span class="p">))</span>
   
</pre></td></tr></tbody></table></code></pre></figure>

<p>Note that the df[‘name’] column contains both ‘name’ and ‘brand_name’ features concatenated and preprocessed, similarly df[‘text’] feature contains ‘item_description’ and ‘category_name’ features concatenated and preprocessed.</p>

<p>Let’s proceed to the further processes but before that, we need to split the data into train and cross-validation sets. Also, we’ll be converting the target values i.e. the prices into log form so that they are normally distributed and the RMSLE(root mean squared log error) is easy to compute.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>    <span class="n">df</span> <span class="o">=</span>  <span class="n">df</span><span class="p">[[</span><span class="s">'name'</span><span class="p">,</span> <span class="s">'text'</span><span class="p">,</span> <span class="s">'shipping'</span><span class="p">,</span> <span class="s">'item_condition_id'</span><span class="p">]]</span>
    <span class="n">X_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[[</span><span class="s">'name'</span><span class="p">,</span> <span class="s">'text'</span><span class="p">,</span> <span class="s">'shipping'</span><span class="p">,</span> <span class="s">'item_condition_id'</span><span class="p">]]</span>

    <span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
    <span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

    <span class="n">y_scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
    <span class="n">X_train</span><span class="p">,</span> <span class="n">X_cv</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_cv</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
                                   <span class="n">test_size</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
    <span class="n">y_train_std</span> <span class="o">=</span>  <span class="n">y_scaler</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">log1p</span><span class="p">(</span><span class="n">y_train</span><span class="p">.</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>Now it’s time to convert these preprocessed text features into a numerical representation. I’ll be using TF-IDF vectorizer for this process. We’ll start with the feature ‘name’</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="code"><pre>    <span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">TfidfVectorizer</span> <span class="k">as</span> <span class="n">Tfidf</span>

    <span class="n">tfidf</span> <span class="o">=</span> <span class="n">Tfidf</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">350000</span><span class="p">,</span> <span class="n">token_pattern</span><span class="o">=</span><span class="s">'\w+'</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span> <span class="c1"># using only top 350000 tf-idf features (with bi-grams).
</span>    <span class="n">X_tr_name</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s">'name'</span><span class="p">])</span>
    <span class="n">X_cv_name</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_cv</span><span class="p">[</span><span class="s">'name'</span><span class="p">])</span>
    <span class="n">X_test_name</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s">'name'</span><span class="p">])</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>Next comes the feature ‘text’</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
</pre></td><td class="code"><pre>    <span class="n">tfidf</span> <span class="o">=</span> <span class="n">Tfidf</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">350000</span><span class="p">,</span> <span class="n">token_pattern</span><span class="o">=</span><span class="s">'\w+'</span><span class="p">,</span> <span class="n">ngram_range</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span> <span class="c1"># using only top 350000 tf-idf features (with tri-grams).
</span>    <span class="n">X_tr_text</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s">'text'</span><span class="p">])</span>
    <span class="n">X_cv_text</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_cv</span><span class="p">[</span><span class="s">'text'</span><span class="p">])</span>
    <span class="n">X_test_text</span> <span class="o">=</span> <span class="n">tfidf</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s">'text'</span><span class="p">])</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>Let’s also process the remaining categorical features starting with ‘shipping’
since this feature takes only 2 values 0 and 1, we do not need to perform some special kind of encoding for these, let’s keep them as they are.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>    <span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">sparse</span>
    <span class="n">X_tr_ship</span> <span class="o">=</span>
             <span class="n">sparse</span><span class="p">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s">'shipping'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">X_cv_ship</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">X_cv</span><span class="p">[</span><span class="s">'shipping'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">X_test_ship</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s">'shipping'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>The second categorical feature that also happens to be an ordinal feature is ‘item_condition_id’. Remember these can take 5 integer values (1–5) so we’ll also keep these as they are.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>    <span class="n">X_tr_condition</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s">'item_condition_id'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span>
    <span class="n">X_cv_condition</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">X_cv</span><span class="p">[</span><span class="s">'item_condition_id'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span>
    <span class="n">X_test_condition</span> <span class="o">=</span> <span class="n">sparse</span><span class="p">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="s">'item_condition_id'</span><span class="p">].</span><span class="n">values</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mf">1.</span><span class="p">)</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>Notice that I’ve used -1 because this feature contains 5 types of values between (1–5) so -1 converts them to a range of (0–4). This will give us an advantage while converting to sparse data.</p>

<p>Now as the final step, we’ll be stacking these features column-wise.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3052/1*NaNK4CSTlS6OcNde3_Y7-A.png" />
    </div>
</div>

<p>I will now convert this preprocessed data into a binary form in which the values will only be either 1s or 0s.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>    <span class="n">X_tr_binary</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_tr</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">X_cv_binary</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_cv</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">X_test_binary</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_test</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p>The advantage of this step is that now we’ll be having 2 datasets with a good variance to work on.</p>

<p><img src="" alt="" />
<strong><em>Modeling</em></strong></p>

<p>It’s time for testing some models on our data. The models that we’ll be trying are-</p>
<ul>
  <li>Ridge regressor</li>
  <li>Linear SVR</li>
  <li>SGD Regressor</li>
  <li>Random Forest Regressor</li>
  <li>Decision Tree Regressor</li>
  <li>XGBoost Regressor</li>
</ul>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Ridge Regressor on normal data</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*eeIvlwkMNG1wSmj3FR6M2g.gif" />
    </div>
</div>

<p>We use linear regression to find the optimal hyperplane (the red line in the above gif) such that the <strong>loss</strong> or square of the sum of distances of each point from the plane/line is minimum. We can notice that the loss will be minimum if we consider the line obtained at iterations=28.
Ridge regression is also known as Linear Regression with L2 Regularization which means it uses the sum of the square of weights as a penalty. The penalty term is added to restrict the model from overfitting (capturing noise).
The Ridge regression has just 1 hyperparameter <strong>λ</strong> that is multiplied with the penalty/regularization term and it decides the degree of underfitting the model undergoes. The greater the value of λ, the more we under-fit.
alpha is simply the regularization strength and it must be a positive float. So as alpha increases, the underfitting also increases.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2604/1*N1cZF0vMWfRxbVadoXRPtQ.png" />
    </div>
</div>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'train loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'test loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'alpha VS RMSLE-loss plot'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Hyperparameter: alpha'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'RMSLE loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*AOCULXcFu5oHAtQOuUbW0A.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>
    <p>We can observe that as alpha decreases, the model starts overfitting.</p>
  </li>
  <li>
    <p>The test loss is minimum at alpha=1.</p>
  </li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3488/1*-guTkXjJVsoTAUnWqAzXXA.png" />
    </div>
</div>

<p>Okay, so our Ridge returned a loss of 0.4232 on cv data.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Ridge Regressor on binary data</strong></p>
</blockquote>

<p>Now we’ll be using the Ridge regressor on the binary data</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'train loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha_list</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'test loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'alpha VS RMSLE-loss plot (on binary features)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Hyperparameter: alpha'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'RMSLE loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*u9RLEGtoW6t8CNAdP7zy-Q.png" />
    </div>
</div>

<ul>
  <li>We can observe that the loss is minimum at alpha=100.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3256/1*jfkNabTLvD_u3bBkTzqA9w.png" />
    </div>
</div>

<p>Our Ridge regressor returned a loss of 0.4335 on cv data.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Let’s Try SGD-Regressor (as SVR) on Binary data</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2400/1*wsBakfF2Geh1zgY4HJbwFQ.gif" />
    </div>
</div>

<p><img src="" alt="" />
Let’s quickly refresh what SGD is and how it works. Remember the loss that I mentioned in Ridge regression? Well, there are different types of losses, let’s understand this geometrically. If a regression problem is all about finding the optimal hyperplane that best fits our data, a loss simply means how much our data differs from the hyperplane. So, a low loss means that the points don’t differ much from our hyperplane and the model performs well and vice-versa.
In the case of linear regression, the loss is a squared loss and it is obtained by taking the sum of squared distances of data points from the hyperplane divided by the number of terms.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*RRHnDdeeoCMg52HZ.png" />
    </div>
</div>

<p><img src="" alt="" />
Loss functions are important because they define what the hyperplane will be like. There are other algorithms called the Gradient Descent that make use of these loss-functions and update the parameters of the hyperplane such that it perfectly fits the data. The goal here is to minimize the loss. SGD is one optimized algorithm that updates the parameters of the hyperplane by reducing the loss step by step. It is done by calculating the gradient of the loss function with respect to the features and then using those gradients to descent towards the minima. In the above diagram (left part), we can see how the algorithm is reaching the minima of loss function by taking the right step downhill, and with each step in the correct direction, the parameters are getting updated which leads to a better fitting hyperplane (right part). To know more about the Stochastic Gradient Descent (SGD) algorithm you can check <a href="https://www.pyimagesearch.com/2016/10/17/stochastic-gradient-descent-sgd-with-python/">this wonderful blog</a>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*xSejSNMd5xAXxE5M.gif" />
    </div>
</div>

<p>Here are some other common losses but we’ll be using ‘Huber’, ‘epsilon_insensitive’, and ‘squared_epsilon_insensitive’ for the hyperparameter tuning of this model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2480/1*I__GS6M_0shj0oEMEJfNBQ.png" />
    </div>
</div>

<p>The random search cross-validation tells us that ‘squared_epsilon_insensitive’ loss with L2 regularization works best for this data. By the way, ‘squared_epsilon_insensitive’ loss is one of the losses used by another well-known machine learning algorithm Support Vector Machine which uses margin maximization technique by making the use of support vectors to generate a better fitting hyperplane.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*6oNO9vFNwBL91iQL.jpeg" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>In this diagram, the dotted lines are called decision boundaries, and the points lying on the dotted lines are called support vectors and the objective of SVR is to maximize the distance between these decision boundaries.</li>
</ul>

<p>But why is margin maximization so important that it makes SVM one of the top ML algorithms? Let’s quickly understand this using a simple classification problem where we need to find an optimal hyperplane that separates the blue and red points.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*A_zxELRrMOraaHyV.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<ul>
  <li>Look at the two planes in the figure denoted by the names <em>Hyperplane</em> and <em>Optimal Hyperplane</em>. Well anyone can tell that the <em>Optimal Hyperplane</em> is much better at separating the blue and red points than the other plane and using SVM, this <em>optimal hyperplane</em> is almost guaranteed.</li>
</ul>

<p>One fun fact is that the flat bottom part in the ‘squared_epsilon_insensitive’ loss is because of this margin maximization trick. You can refer to <a href="https://medium.com/coinmonks/support-vector-regression-or-svr-8eb3acf6d0ff">this</a> and <a href="https://en.wikipedia.org/wiki/Support_vector_machine">this blog</a> to learn more about SVR.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3872/1*xOwShgO7BXHzmdVWyTrMLw.png" />
    </div>
</div>

<p>SGD Regressor (as SVR) returned a loss of 0.4325… on cv data.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Let’s try SGD regressor (as linear regressor) on binary data</strong></p>
</blockquote>

<p>Here we’ll be performing all the previous step procedures but on binary data.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2220/1*OpMLYa_iEokmFj8e4TGMaw.png" />
    </div>
</div>

<p>The random search cross-validation tells us that ‘squared_loss’ loss with L2 regularization works best for this data. By the way, this setup of squared_loss with L2 regularization sounds familiar right? This is exactly what we used in the Ridge regression model. Here we are approaching this from an optimization problem’s perspective because SGDRegressor gives us much more hyperparameters to play around with and fine-tune our model.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3636/1*fNDaUXTXcyFWSfKpOKnW9w.png" />
    </div>
</div>

<p>SGD Regressor (as linear regressor) returned a loss of 0.4362 on cv data.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Linear SVR on normal data</strong></p>
</blockquote>

<p>Let’s try Support Vector Regressor on normal data. The hyperparameter here is C that is also the reciprocal of alpha which we discussed in Ridge regression.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'train loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'test loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'alpha VS RMSLE-loss plot (on binary features)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Hyperparameter: C (1/alpha)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'RMSLE loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*4-J5Ji1ADJ27YFz0u2lMlQ.png" />
    </div>
</div>

<ul>
  <li>We can see that 0.1 is the best hyperparameter value of hyperparameter C that gives us the minimum test loss.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3444/1*L35e-55NWoVP3OjtwJNSJg.png" />
    </div>
</div>

<p>Linear SVR returned a loss of 0.4326 on the CV of normal data.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Linear SVR on binary data</strong></p>
</blockquote>

<p>Now we’ll try Support Vector Regressor on binary data. The hyperparameter here is again C that is also the reciprocal of alpha which we discussed in Ridge regression.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">train_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'train loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'test loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'alpha VS RMSLE-loss plot (on binary features)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Hyperparameter: C (1/alpha)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'RMSLE loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*2XsJL6XbIwDKWNpesnFvfg.png" />
    </div>
</div>

<ul>
  <li>We can see that 0.01 is the best hyperparameter value of hyperparameter C that gives us the minimum test loss.</li>
</ul>

<p>Linear SVR returned a loss of 0.4325 on the cv of binary data.</p>

<p><img src="" alt="" /></p>
<h3 id="tree-based-models">Tree-based models</h3>

<p><strong><em>The tree-based models below were taking too much time to fit (more than 60 mins) so I reduced the features using Ridge regressor on binary data.</em></strong></p>

<p>Note: Another dimensionality technique that I tried was truncated-SVD but it required a lot of RAM (more than 16 GB) for computation and since this is a kernel challenge, using the complete data didn’t make much sense.</p>

<p><strong><em>Selecting top features for tree-based models:</em></strong></p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="code"><pre>    <span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFromModel</span>

    <span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDRegressor</span>
    <span class="n">regressor</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
    <span class="n">selection</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">regressor</span><span class="p">)</span>
    <span class="n">selection</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_tr_binary</span><span class="p">,</span> <span class="n">y_train_std</span><span class="p">.</span><span class="n">ravel</span><span class="p">())</span>

    <span class="n">X_train_top</span> <span class="o">=</span> <span class="n">selection</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_tr_binary</span><span class="p">)</span>
    <span class="n">X_cv_top</span> <span class="o">=</span> <span class="n">selection</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_cv_binary</span><span class="p">)</span>
    <span class="n">X_test_top</span> <span class="o">=</span> <span class="n">selection</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test_binary</span><span class="p">)</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Decision Tree</strong></p>
</blockquote>

<p>Our first tree-based model is a Decision Tree, before using this on our dataset, let’s first quickly understand how it works.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://miro.medium.com/proxy/0*cant-HQdfMju-GxG" />
    </div>
</div>

<p><img src="" alt="" />
Decision Trees are made up of simple if-else statements and using these conditions they decide how to predict the price of a product given its name, conditioning, etc. Geometrically speaking, they fit on the data using several hyperplanes that are parallel to the axes.
While training a tree, the tree learns these if-else statements by using and verifying the train data. And when it is trained, it uses these learned if-else conditions to predict the value of test data.
But how does it decide how to split the data or what feature to consider while splitting the data and construct a complete tree?
Well, it uses something called entropy which is a measure of certainty to construct the tree.
Decision trees have several hyperparameters but we’ll be considering only the 2 important ones-</p>
<ul>
  <li><em>max_depth:</em> It denotes the maximum depth of a decision tree. So if the max_depth is supposed 4, while training, the tree constructed will not have a depth more than 4.</li>
  <li><em>min_samples_split:</em> It denotes the minimum number of data points that must be present to perform a split or consider an if-else condition on it. So if the min_samples_split is supposed 32, while training, the tree constructed will not apply an if-else condition if it sees less than 32 data points.</li>
</ul>

<p>Both the above hyperparameters restrict a decision tree from either underfitting or overfishing. A high max_depth and a low min_samples_split value makes decision trees more prone to overfitting and vice-versa.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*DzCdcC-xomLcLd1B.png" />
    </div>
</div>

<ul>
  <li>
    <p>In this figure, we can see how a trained decision tree algorithm tries to fit on the data, notice how the fitting lines are made up of axes parallel lines.</p>
  </li>
  <li>
    <p>We can also notice how a decision tree with a greater value of max_depth is prone to capture noisy points also.</p>
  </li>
</ul>

<p>I will not go into the internal working of decision trees in this blog since it will make it too long, to learn more about the internal working of a decision tree, you can check out this awesome <a href="https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8">blog</a>.</p>

<p>Let’s perform some hyperparameter tuning on our decision tree using RandomSearchCV and check what are the best hyperparameters for our tree.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2988/1*_ZinFL3ECLWXGNC6Lo768A.png" />
    </div>
</div>

<p>The best hyperparameter values returned are max_depth=64 and min_samples_split = 64. Now let’s check the loss obtained after training a decision tree on these hyperparameters.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3736/1*0cbEiS0XSgNC82nrDjfkaw.png" />
    </div>
</div>

<p>The loss values are not that great given that it took 14 mins to train. Our Linear models have outperformed the decision tree model by far.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Random forest — (max_depth=3, n_estimators=100)</strong></p>
</blockquote>

<p>Now let’s use another awesome tree-based model or I should say models to model our data.
Random forests are ensembles that are made up of multiple models. The idea is to use random parts of the data to train multiple models and then use the average predictions from these multiple models as the final value. This makes sense because of training several models using random parts of complete data creates models that are to some extent biased in different ways. Now by taking the average prediction from all these models, in the end, results in a better-predicted value.</p>

<p>The name Random Forest comes from Bootstrap sampling which we use in sampling data <em>randomly</em> from the training dataset and since we use multiple decision trees as our base models, it has the word <em>forest</em>.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2612/0*N2HUznk6Rrtpg1iR.png" />
    </div>
</div>

<p>The above diagram denotes how Random Forest trains different base learners denoted as Tree 1, Tree 2, … using randomly sampled data and then collects and averages the predictions from these trees.</p>

<p>Random Forest has multiple hyperparameters but for our data, we’ll be using just 2:
<em>- n_estimator:</em> this denotes the number of base models that we want our random forest model to have.
<em>- max_depth:</em> This denotes the maximum depth of each base model i.e. the decision tree.</p>

<p>Let’s train a random forest model and perform some hyperparameter tuning on it.</p>

<p>The training time for this model was about 23 mins.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3136/1*8ehM9Y9BQGveU8-yBmny4A.png" />
    </div>
</div>

<p>We can see that this model does not perform well on the given dataset and the results are not at all good.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Random forest — (max_depth=4, n_estimators=200)</strong></p>
</blockquote>

<p>Here I’ve used the same model but with some changes in the architecture.
I’ve increased the max_depth to 4 and the number of base learners to 200.
Let’s see how the model performs.</p>

<p>The training time for this model was about 65 mins.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2988/1*7_d_JkciaKqEK7U04Zyzeg.png" />
    </div>
</div>

<p>The results are slightly better than the previous Random forest model but still not even close to our Linear models.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>XGBoost — (max_depth=4, n_estimators=200)</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*CVSyne5ZJ7MnDeAU.gif" />
    </div>
</div>

<p>This is the final tree-based model that we’ll be trying and it is called XGBoost.
XGBoost is a slightly enhanced version of GBDT which again is an ensemble modeling technique. In Gradient boosting, the purpose is to reduce the variance or reduce the underfitting behavior on a dataset. Let’s see how it works.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/0*ctuJT4dyWDY18QCm.png" />
    </div>
</div>

<p>In GBDT, we start by training our first base model which is typically a high bias decision tree using the train data, then we take the predicted values from this model and calculate the error which is defined by how much the predictions differ from the actual values. Now we train our second base learner but this time instead of using only the train data, we use also use the error obtained from our first base learner and again we take the predicted values from this model and calculate the error. This goes on till all the base learners are covered and as we train the base learners one by one, we notice that the error value slowly diminishes. You can read more about GBDT <a href="http://uc-r.github.io/gbm_regression">here</a>.
XGBoost is a slightly modified version of GBDT and it uses techniques like row sampling and column sampling which are techniques from Random Forest to construct the base-learners.</p>

<p>Let’s quickly check out the code for XGBoost, I’ll be using 2 hyperparameters:</p>
<ul>
  <li>n_estimators: which denotes the number of base-learners which are decision tree models.</li>
  <li>max_depth: which denotes the maximum depth of the base learner decision tree.</li>
</ul>

<p>The model took about 27 mins to train.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3012/1*GpBSeek8lu_plTioQc1r0Q.png" />
    </div>
</div>

<p>The results are not as bad as random forest but not as good as linear models also.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>XGBoost — (max_depth=6, n_estimators=500)</strong></p>
</blockquote>

<p>Let’s try XGBoost with max_depth=6 and n_estimators=500.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3400/1*Fw1yJ3iF2rCJAn1O5vz9_Q.png" />
    </div>
</div>

<p>We can see a decent amount of improvement from the previous model but it took the model 78 mins to train.</p>

<h3 id="lets-compare-the-different-models-and-their-performance">Let’s compare the different models and their performance:</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4636/1*P--evq3wbxVgY9Tj7787OQ.png" />
    </div>
</div>

<p><img src="" alt="" />
In the above table, we can see that the tree-based models are taking too much time to compute, in fact, the data I’m using for tree-based is much smaller, I’m using only the top selected binary features from Ridge regressor. So the new data has only around 236k features instead of the original 700k that other linear models are trained on. We can also observe that the minimum loss on cross-validation data that we were able to obtain is 0.4232… let’s try to reduce this further using ensemble modeling.</p>

<p><strong>The Linear models have outperformed other tree-based models so I’ll be using these to create an Ensemble.</strong></p>

<p>Let’s concatenate the results obtained from the top 6 linear models.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3776/1*eAS9FC7SP5MMTqR4eOnG_Q.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4440/1*QrZtzLNhER9TszchVkSq5w.png" />
    </div>
</div>

<p>Now let’s quickly test a simple ensemble that takes these features as input and computes the output as a mean of these values.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3148/1*dg9VBOEZjOnmC7TFwOpGXA.png" />
    </div>
</div>

<p>We can observe that the loss has increased slightly, which means that this method alone is not decent enough to produce good scores.</p>

<p>Now let’s check the correlation between these new features because all of them are from linear models and produce a similar loss. If they are heavily correlated, they will not improve the overall loss much.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s">'y_pred_ridge_binary_tr'</span><span class="p">,</span> <span class="s">'y_pred_ridge_normal_tr'</span><span class="p">,</span>
               <span class="s">'y_pred_svr_normal_tr'</span><span class="p">,</span><span class="s">'y_pred_svr_binary_tr'</span><span class="p">,</span>
               <span class="s">'y_pred_sgd_lr_binary_tr'</span><span class="p">,</span> <span class="s">'y_pred_sgd_svr_binary_tr'</span><span class="p">]</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">y_pred_tr_ensemble</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">Var_Corr</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">corr</span><span class="p">()</span>
    <span class="n">sns</span><span class="p">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">Var_Corr</span><span class="p">,</span> <span class="n">xticklabels</span><span class="o">=</span><span class="n">Var_Corr</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span>
    <span class="n">yticklabels</span><span class="o">=</span><span class="n">Var_Corr</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Correlation between different features.'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2432/1*w4KjE92siYvJcKt61675KQ.png" />
    </div>
</div>

<ul>
  <li>We can see that the results from the underlying models are heavily correlated so there isn’t much scope of getting a marginally well score from building an ensemble on them.</li>
</ul>

<p>To tackle this, I increased the dimensionality of this data by adding the top features that were gathered from the Linear model on binary data that we used to train the tree-based models.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3372/1*DxQH9MNQ5TsMzAS50wiysw.png" />
    </div>
</div>

<p>It’s time to try different models on these newly generated features to see if we can improve the loss.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Let’s try SGD Regressor using different hyperparameters</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2684/1*YI8aWb3zJL_p67qa_FmJHw.png" />
    </div>
</div>

<p>The above code block represents the best hyperparameters returned by RandomSearchCV.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3444/1*s24NGK5ZOPD1mOvflcQA_w.png" />
    </div>
</div>

<p>The CV loss is not up to the mark since we already have a loss of 0.4232… and we are looking for a loss lower than that.</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Let’s try Linear SVR and Ridge regressor on the new features</strong></p>
</blockquote>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">ridge_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">ridge_loss</span><span class="p">)</span>
    <span class="n">linearsvr_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">linearsvr_loss</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">ridge_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'Ridge train'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">ridge_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'Ridge test'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linearsvr_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'linearsvr train'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linearsvr_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'linearsvr test'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Hyperparameter: alpha or (1/C)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Linear SVR and Ridge losses'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*yaalDojG9iH1xgmnkYI89A.png" />
    </div>
</div>

<ul>
  <li>We can see that at alpha=100000, the cv loss returned by Ridge regressor and Linear SVR are minimum. Let’s fit the models on that.</li>
</ul>

<p>Training a Ridge regressor with alpha = 100000</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3228/1*Tdz-S5tXH1WjRklGmA9xjA.png" />
    </div>
</div>

<p>Training a Linear SVR with C = 0.00001</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3268/1*-0KihDZg6yACp0SJoVs9zg.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3724/1*Jcy10PEYybzNB06Lmuo03Q.png" />
    </div>
</div>

<p><img src="" alt="" />
Okay, by looking at the above table we can tell that the Ridge and LinearSVR models yield the best results, so we’ll be using these to generate one more and the final layer of our ensemble.</p>

<p>Let’s quickly fit the data using these models and concatenate the output that we’ll feed as input to our final ensemble layer.</p>

<p>We’ll now create the final layer of our ensemble using the generated output from the previous layer models. We’ll be using some linear models but before that, let’s test the simple mean results.</p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3344/1*s2SWaJ2bbnKGvM6yMHX9zg.png" />
    </div>
</div>

<p>The results are better than the LinearSVR model alone but the Ridge still outperforms every model till now.</p>

<p>Let’s try some linear models now for the final layer:</p>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>SGD Regressor</strong></p>
</blockquote>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2432/1*UnARXx06nsf4Ipi8IljAMQ.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4188/1*rlb1u_EZ5kasq7Cg0gCvGw.png" />
    </div>
</div>

<p><img src="" alt="" /></p>
<blockquote>
  <p><strong>Let’s try Ridge and Linear-SVR as the final layer model</strong></p>
</blockquote>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><table class="rouge-table"><tbody><tr><td class="gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre></td><td class="code"><pre>    <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
    <span class="n">ridge_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">ridge_loss</span><span class="p">)</span>
    <span class="n">linearsvr_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">linearsvr_loss</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">ridge_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'Ridge train'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">ridge_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'Ridge test'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linearsvr_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'linearsvr train'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">linearsvr_loss</span><span class="p">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'linearsvr test'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'Hyperparameter: alpha or (1/C)'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'loss'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">xscale</span><span class="p">(</span><span class="s">'log'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Linear SVR and Ridge losses'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
    
</pre></td></tr></tbody></table></code></pre></figure>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/2000/1*KiLPm2CRbWW-auX7ua3Vgg.png" />
    </div>
</div>

<ul>
  <li>The results are close but the Ridge regressor outperforms LinearSVR.</li>
</ul>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3360/1*CpaKlAOcl47fhvVkVGyGXg.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3720/1*uWD04L7O3hi423rS4sFc6Q.png" />
    </div>
</div>

<p><img src="" alt="" />
<strong>Here are all the models that have been used for the ensemble, compared in a tabular form.</strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3736/1*CyA0T6y9RSgWS0a3JFp5ZQ.png" />
    </div>
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3952/1*QLU58lo_X3qE_HLb70MTmg.png" />
    </div>
</div>

<p><img src="" alt="" />
<strong><em>Finally, let’s predict the prices of the test dataset and check how our ensemble performs on the Kaggle leaderboard.</em></strong></p>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/3680/1*Q4xHaJNty0OCsrqN4f_ecQ.png" />
    </div>
</div>

<h3 id="after-submitting-the-predicted-results-i-was-able-to-obtain-a-score-of-042457-that-corresponds-to-the-top-6-on-the-kaggle-leaderboard">After submitting the predicted results, I was able to obtain a score of 0.42457 that corresponds to the top 6% on the Kaggle leaderboard.</h3>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-1">
        <img class="img-fluid rounded z-depth-0" src="https://cdn-images-1.medium.com/max/4076/1*m8PVQnO4QZhA6hev8Lfl2w.png" />
    </div>
</div>

<p><img src="" alt="" /></p>

<p><strong><em>Future work</em></strong></p>

<ul>
  <li>
    <p>The problem can be solved using deep learning algorithms like GRU, MLP, BERT as most of the features are from text descriptions. We can try these state of the art techniques on this data and try to improve the scores.</p>
  </li>
  <li>
    <p>The code has not been optimized for multiprocessing such that it uses all the 4 CPUs. I think that is worth a try because then even the Tree-based models could be added to the ensemble.</p>
  </li>
</ul>

<p><strong><em>References</em></strong></p>

<ul>
  <li>
    <p><a href="https://www.kaggle.com/c/mercari-price-suggestion-challenge/discussi">https://www.kaggle.com/c/mercari-price-suggestion-challenge/discussi</a> on/50256</p>
  </li>
  <li>
    <p><a href="https://www.youtube.com/watch?v=QFR0IHbzA30">https://www.youtube.com/watch?v=QFR0IHbzA30</a></p>
  </li>
  <li>
    <p><a href="https://youtu.be/_PwhiWxHK8o">https://youtu.be/_PwhiWxHK8o</a></p>
  </li>
  <li>
    <p><a href="https://youtu.be/UHBmv7qCey4">https://youtu.be/UHBmv7qCey4</a></p>
  </li>
  <li>
    <p><a href="https://www.appliedaicourse.com/">https://www.appliedaicourse.com/</a></p>
  </li>
</ul>

<p><strong><em>Final note</em></strong></p>

<p>Thank you for reading the blog. I hope it was useful for some of you aspiring to do projects on machine-learning, ensemble modeling, data processing, data visualizing.</p>

<p>And if you have any doubts regarding this project, please leave a comment in the response section or in the GitHub repo of this project.</p>

<p>The full project is available on my Github:
<a href="https://github.com/SarthakV7/mercari_kaggle">https://github.com/SarthakV7/mercari_kaggle</a>
Find me on LinkedIn: <a href="http://www.linkedin.com/in/sarthak-vajpayee">www.linkedin.com/in/sarthak-vajpayee</a></p>

<p>Peace! ☮</p>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2021 Sarthak  Vajpayee.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank">Unsplash</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/my_folio/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/my_folio/assets/js/common.js"></script>


</html>
